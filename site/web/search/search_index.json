{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Joel Vandiver's Blog","text":"<p>I found the best way to learn something is to try and explain it.  </p> <p>I intend to use this blog for learning.  I started out my career as a high school Mathematics teacher.  I thought I knew Mathematics after I graduated college, but I've never been more lost than standing in front of 30 high school students trying to teach them Algebra I.  I had already completed Cal I, II, &amp; III, Diff. Eq., and Linear Algebra, and yet I struggled to convey basic concepts to them.  Sure, I could ace an Algebra I test - no problem - but I struggled to meet students where they were and in how they learned best. It took repetition in teaching and studying for me to unlock the richness of the subject so that I could meet the students where they were.  </p> <p>The image to the right was generated by Dall-E 3.</p>"},{"location":"about/","title":"About Me","text":"<p>I started programming in 2008 with JavaScript as a teacher to build a website for my students.  By this point, I had flipped my classroom where students would learn from my online videos (see my original youtube account) and then they completed \"homework\" in the classroom with me to help as their tutor.  </p> <p>The deeper I went with programming, the more successful I became at teaching Mathematics.  I began generating individualized tests based on abstract representations of Math problems.  I had even gotten to the point where I started learning C++!  I figured, \"Eh, anyone who's serious about programming has to learn C++.\"  </p> <p>Then, an opportunity came my way to work professionally as a programmer.  I started my career in .NET.  Day after day, I would slog through raw SQL queries as I learned to express my questions with code.  I was shocked by the impurities of imperative languages such as C#. Coming from a Math background, I had exclusively programmed in a functional style.  It wasn't until years later that I learned just how impure JS actually is.</p> <p>Later, I had the opportunity to move into a position at a bigger company with a larger space to grow in.  Over time, I have worked in various positions in the company, and I've had the opportunity to learn from other languages and frameworks.</p> <ul> <li>C# .NET Taught me object orientated programming, dependency injection, and a whole host of design patterns.</li> <li>F# .NET Taught me to rely on immutability by default, use functions as first-class values, and the benefits of minimizing side-effects.</li> <li>TypeScript Taught me that types can be computable as well.</li> <li>Python Taught me to get things done and that even dynamic languages can support annotations.</li> <li>Rust Taught me how to reason about memory and how to think of data structures at a systems level.</li> <li>Docker Taught me to formally define my application's environment and to explore other technologies.</li> <li>Kubernetes Taught me to distribute my services to simplify development at scale.</li> </ul> <p>My career has been a wild ride of constant learning, and I've got so much more to explore!</p> <p>Enjoy!</p> <p>P.S.  If you have any questions or feedback for me, then please feel free to look me up on LinkedIn.</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2019/04/06/basic-f-expecto-example/","title":"Basic F# Expecto Example","text":"<p>First, let's gather our dependencies.</p>"},{"location":"blog/2019/04/06/basic-f-expecto-example/#define-a-simple-test-case","title":"Define a simple test case.","text":"<pre><code>#r @\"C:\\Users\\joelv\\.nuget\\packages\\NETStandard.Library\\2.0.3\\build\\netstandard2.0\\ref\\netstandard.dll\"\n#r @\"C:\\git\\joelvandiver.com\\packages\\Expecto\\lib\\netstandard2.0\\Expecto.dll\"\n\nopen Expecto\n\nlet toBeTest =\n  testCase \"To be or not to be\" &lt;| fun () -&gt;\n    let toBe = true\n    Expect.isTrue toBe \"You do not exist.\"\n\nrunTests defaultConfig toBeTest\n</code></pre> <p>Output: <pre><code>[15:36:13 INF] EXPECTO? Running tests... &lt;Expecto&gt;\n[15:36:13 INF] EXPECTO! 1 tests run in 00:00:00.0109563 for To be or not to be \u2013 1 passed, 0 ignored, 0 failed, 0 errored. Success! &lt;Expecto&gt;\nval toBeTest : Test =\n  TestLabel\n    (\"To be or not to be\",TestCase (Sync &lt;fun:toBeTest@5-1&gt;,Normal),Normal)\nval it : int = 0\n</code></pre></p>"},{"location":"blog/2019/04/06/basic-f-expecto-example/#combine-multiple-tests-into-a-list","title":"Combine multiple tests into a list.","text":"<pre><code>let toThinkTest =\n  test \"I think therefore I am.\" {\n    let iThink = true\n    Expect.isTrue iThink \"You do not think so you do not exist.\"\n  } \n\nlet existenceTests = testList \"Test for Existence\" [toBeTest; toThinkTest]\n\nrunTests defaultConfig existenceTests\n</code></pre> <p>Output: <pre><code>[15:36:46 INF] EXPECTO? Running tests... &lt;Expecto&gt;\n[15:36:47 INF] EXPECTO! 2 tests run in 00:00:00.0219511 for Test for Existence \u2013 2 passed, 0 ignored, 0 failed, 0 errored. Success! &lt;Expecto&gt;\nval toThinkTest : Test =\n  TestLabel\n    (\"I think therefore I am.\",TestCase (Sync &lt;fun:toThinkTest@11-1&gt;,Normal),\n     Normal)\nval existenceTests : Test =\n  TestLabel\n    (\"Test for Existence\",\n     TestList\n       ([TestLabel\n           (\"To be or not to be\",TestCase (Sync &lt;fun:toBeTest@5-1&gt;,Normal),\n            Normal);\n         TestLabel\n           (\"I think therefore I am.\",\n            TestCase (Sync &lt;fun:toThinkTest@11-1&gt;,Normal),Normal)],Normal),\n     Normal)\nval it : int = 0\n</code></pre></p>"},{"location":"blog/2019/04/06/test-with-clone-and-generator/","title":"Test with Clone and Generator","text":"<p>One of the primary goals of each unit test is to ensure that the test code is as decoupled from the source code as possible.  Usually, the test should only have code specific to its concerns.  </p> <p>Key Problem: Generating test data is usually where the largest amount of coupling to production code exits.</p> <p>Let's setup some code to test.</p> <pre><code>open System\n\ntype Role =\n| Senator\n| Jedi\n| Sith\n\ntype Character =\n    {   id          : Guid\n        firstname   : string\n        lastname    : string\n        role        : Role\n        }\n\nlet isSithLord character = character.role = Sith\n</code></pre> <p>Requirement:  isSithLord should return true if the role is Sith.</p>"},{"location":"blog/2019/04/06/test-with-clone-and-generator/#creating-test-data-in-the-test","title":"Creating Test Data in the Test","text":"<pre><code>let testA = \n    let palpatine =\n        {   id          = Guid.NewGuid()\n            firstname   = \"Darth\"\n            lastname    = \"Sidious\"\n            role        = Sith\n            }\n    printfn \"%A\" palpatine\n    let actual = isSithLord palpatine\n    printfn \"Palpatine is a Sith Lord:  %b\" actual\n    actual\n</code></pre> <p>Output: <pre><code>{id = 489c7a10-f872-4cfa-a79f-cf0e48fbb2d1;\n firstname = \"Darth\";\n lastname = \"Sidious\";\n role = Sith;}\nPalpatine is a Sith Lord:  true\nval testA : bool = true\n</code></pre></p> <p>Key Points:</p> <ol> <li>The data for the test (palpatine) was created within the test itself.  </li> <li>The only necessary data to test is that the role of Palpatine is Sith.</li> </ol>"},{"location":"blog/2019/04/06/test-with-clone-and-generator/#abstracting-test-data-creation-into-a-generator","title":"Abstracting Test Data Creation into a Generator","text":"<pre><code>let generateCharacter () =\n    {   id          = Guid.NewGuid()\n        firstname   = \"Some First Name\"\n        lastname    = \"Some Last Name\"\n        role        = Senator\n        }\n\nlet testB = \n    let character =\n        { generateCharacter() with  \n            role = Sith\n            }\n    printfn \"%A\" character\n    let actual = isSithLord character\n    printfn \"A Sith Character is a Sith Lord:  %b\" actual\n    actual\n</code></pre> <p>Output: <pre><code>{id = 1aa8abac-57e4-4fcd-ac15-f0f05509f8cf;\n firstname = \"Some First Name\";\n lastname = \"Some Last Name\";\n role = Sith;}\nA Sith Character is a Sith Lord:  true\nval generateCharacter : unit -&gt; Character\nval testB : bool = true\n</code></pre></p> <p>Key Points:</p> <ol> <li>Generating the data has been abstracted in a utility function.</li> <li>testB is only coupled to the role field of the Character type.</li> <li>The test data is more generic in that it doesn't reference a specific character such as Palpatine.</li> </ol> <p>Abstracting generation of test data can go further by generating all strings with a Random string generator.  This will aide in testing uniqueness of a given string.</p>"},{"location":"blog/2019/06/18/implicit-vs-explicit-type-declarations/","title":"Implicit vs Explicit Type Declarations","text":""},{"location":"blog/2019/06/18/implicit-vs-explicit-type-declarations/#implicit","title":"Implicit","text":"<p>F# supports implicit type declarations.  This can be a powerful technique because the F# type system will:</p> <ol> <li>Infer the Type - Inference allows for more succinct code with less <code>code noise</code>.</li> <li>Auto-Generalize Variable Types - Auto-Generalization leads to more efficient <code>code reuse</code>.</li> </ol>"},{"location":"blog/2019/06/18/implicit-vs-explicit-type-declarations/#inference-example","title":"Inference Example:","text":"<pre><code>let inf1 x = 3 * x\n</code></pre> <p>Output: <pre><code>val inf1 : x:int -&gt; int\n</code></pre> Note:  The <code>x</code> has been inferred to be of type <code>int</code>.  The F# compiler is smart enough to know that only an <code>int</code> parameter type for <code>x</code> will suffice.</p>"},{"location":"blog/2019/06/18/implicit-vs-explicit-type-declarations/#auto-generalization-example","title":"Auto-Generalization Example:","text":"<pre><code>let auto1 x y = x + y\n</code></pre> <p>Output: <pre><code>val auto1 : x:int -&gt; y:int -&gt; int\n</code></pre> Note:  The types for <code>x</code> and <code>y</code> were declared to be of type <code>int</code>.  However, providing an implementation of float values <code>1.</code> and <code>2.</code> values below will resolve the types as <code>float</code>.</p> <pre><code>let auto2 = auto1 1. 2.\n</code></pre> <p>Output: <pre><code>val auto1 : x:float -&gt; y:float -&gt; float\n</code></pre></p> <p>Note:  The FSI environment will throw an error when defining a <code>resolved</code> generic function: Output: <pre><code>index.fsx(6,19): error FS0001: This expression was expected to have type\n    'int'    \nbut here has type\n    'float'  \n</code></pre></p> <p>To avoid the error, send both the <code>auto</code> function value and the computed <code>auto2</code> value to FSI.</p>"},{"location":"blog/2019/07/13/immutability-reduces-information-domain/","title":"Immutability Reduces Information Domain","text":"<p>I have heard many developers who come from non-functional programming languages such as C++ and C# claim that immutability is overly restrictive.  It is true that immutability is restrictive, but this restriction comes at one huge benefit:  reduction of possible outcomes.  </p> <p><code>Information</code> may be informally defined as the element of surprise.  The more surprises a system has, the more information it contains.  Note the distinction between actual information and possible information.  Typically as developers we focus our algorithms on the actual information that our system processes.  But, of equal (or greater) concern is of the possible information the system will proccess.  Let's call this the <code>Information Domain</code>.  Effectively, it is the concern of the developer to process the Information Domain.  We have to anticipate all of the possible outcomes of our system.  </p> <p>Mutability at its core allows for surprise.  </p> <p>Let's take a simple mathematical function and restricted domain for example.</p> <pre><code>let f x = x + 3\nlet domainX = [-2;-1;0;1;2]\nlet table = domainX |&gt; List.map f\n</code></pre> <p>Output: <pre><code>val f : x:int -&gt; int\nval domainX : int list = [-2; -1; 0; 1; 2]\nval table : int list = [1; 2; 3; 4; 5]\n</code></pre></p> <p>Now, let's explore the same function with a mutable side-effect:</p> <pre><code>let mutable X = 4\n\nlet f' x =\n    X &lt;- X + 1\n    X + x + 3\n\nlet table' = domainX |&gt; List.map f'\nlet table'2 = domainX |&gt; List.map f'\n</code></pre> <p>Output: <pre><code>val mutable X : int = 14\nval f' : x:int -&gt; int\nval table' : int list = [6; 8; 10; 12; 14]\nval table'2 : int list = [11; 13; 15; 17; 19]\n</code></pre> Note the <code>table'2</code> has different values from the values in <code>table'</code> as expected.</p> <p>This may seem simple when you can read the source code as we are doing here, but if you did not have access to the source code, this behavior would surely be a surprise.</p> <p>The <code>f'</code> can be restructured to make the behavior less of a surprise:</p> <pre><code>let f'' x y = y + x + 3\n</code></pre> <p>Here, we've removed the mutable call, and instead declared a new param, <code>y</code>, that declares the dependency for <code>f''</code> to operate.  </p> <p>Now, we can provide another restricted domain for the <code>y</code> param:</p> <pre><code>let domainY = [4;5;6;7;8;9]\n</code></pre> <p>This has the effect of combining both <code>domainX</code> and <code>domainY</code> through a cartesian product:</p> <pre><code>let table'3 =\n    domainX\n    |&gt; List.map(fun x -&gt; domainY |&gt; List.map (f'' x))\n    |&gt; List.concat\n</code></pre> <p>Output: <pre><code>val f'' : x:int -&gt; y:int -&gt; int\nval domainY : int list = [4; 5; 6; 7; 8; 9]\nval table'3 : int list =\n  [5; 6; 7; 8; 9; 10; 6; 7; 8; 9; 10; 11; 7; 8; 9; 10; 11; 12; 8; 9; 10; 11;\n   12; 13; 9; 10; 11; 12; 13; 14]\n</code></pre></p> <p>By allowing mutation, we've increased the element of surprise.  I have found this aspect of mutation especially difficult to reason about in large code bases.  </p> <p>Restrictions are not always an inhibitor, rather they can play a central role in improving the speed and effeciency in development.</p>"},{"location":"blog/2019/09/04/nasa-apod/","title":"NASA APOD","text":"<p>I wanted to add something interesting to my blog, and then I remembered that NASA has an astronomy picture of the day (APOD).</p> <p>https://api.nasa.gov/</p> <p>After requesting my own api key, I was able to issue GET requests to get the photo of the day: <code>fetch(\"https://api.nasa.gov/planetary/apod?api_key={API_KEY}\")</code>.</p> <p>Here's a sample from today's response:</p> <p>Json</p> <pre><code>{\n   \"date\": \"2019-09-04\",\n   \"explanation\": \"Will the spider ever catch the fly? Not if both are large emission nebulas toward the constellation of the Charioteer (Auriga).  The spider-shaped gas cloud on the left is actually an emission nebula labelled IC 417, while the smaller fly-shaped cloud on the right is dubbed  NGC 1931 and is both an emission nebula and a reflection nebula.  About 10,000 light-years distant, both nebulas harbor young, open star clusters. For scale, the more compact NGC 1931 (Fly) is about 10 light-years across. The featured picture in scientifically-assigned, infrared colors combines images from the Spitzer Space Telescope and the Two Micron All Sky Survey (2MASS).  Spitzer is celebrating its 16th year orbiting the Sun near the Earth.    APOD in other languages: Arabic, Catalan, Chinese (Beijing), Chinese (Taiwan), Croatian, Czech, Dutch, Farsi, French, French, German, Hebrew, Indonesian, Japanese, Korean, Montenegrin, Polish, Russian, Serbian, Slovenian,  Spanish and Ukrainian\",\n   \"hdurl\": \"https://apod.nasa.gov/apod/image/1909/SpiderFly_Spitzer2Mass_4165.jpg\",\n   \"media_type\": \"image\",\n   \"service_version\": \"v1\",\n   \"title\": \"The Spider Nebula in Infrared\",\n   \"url\": \"https://apod.nasa.gov/apod/image/1909/SpiderFly_Spitzer2Mass_960.jpg\"\n}\n</code></pre> <p>With that response, I simply need to extract the url and create an <code>img</code> element when the <code>media_type === \"image\"</code>.</p> <p>Here's the full example:</p> <p>Html</p> <pre><code>&lt;div id=\"nasa-img-container\"&gt;&lt;/div&gt;\n</code></pre> <p>JavaScript</p> <pre><code>(function () {\n    fetch(\"https://api.nasa.gov/planetary/apod?api_key={API_KEY}\")\n        .then(function (response) {\n            return response.json();\n        })\n        .then(function (json) {\n            if (!json.media_type === \"image\") { return; }\n            var src = json.url;\n            var container = document.getElementById(\"nasa-img-container\");\n            var img = document.createElement('img');\n            img.src = src;\n            container.title = json.explanation;\n            container.appendChild(img);\n        });\n})();\n</code></pre> <p>Here's my favorite photo I've found in APOD so far:</p> <p></p>"},{"location":"blog/2019/09/21/aspnet-core-dependency-injection/","title":"ASP.NET Core Dependency Injection","text":"<p>ASP.NET Core has a nice dependency injection system that works great for object-oriented languages like C#.  F# can be written in an object-oriented way, but is there a way to inject dependencies in a functional style?</p> <p>First off, let's review the F# function composition that comes out-of-the-box.  For example, </p> <pre><code>let f x = x + 3\nlet g x = x + 5\n\nlet fg = f &gt;&gt; g\n\n// Output\nval f : x:int -&gt; int\nval g : x:int -&gt; int\nval fg : (int -&gt; int)\n</code></pre> <p><code>f</code> and <code>g</code> are functions taking one integer and returning another integer.  The output of <code>f</code> is an integer which is the same type of input for <code>g</code>.  This allows us to compose <code>f &gt;&gt; g</code>.</p> <p>Note the sample data below.</p> <pre><code>[1;2;3;4;5] |&gt; List.map fg\nval it : int list = [9; 10; 11; 12; 13]\n</code></pre> <p>Let's expand on the idea to a larger domain than Mathematics.</p> <pre><code>type User = { first: string; last: string }\nlet getUserName (user: User) = user.first + \" \" + user.last\nlet getFirstUser (users: User list) : User = users |&gt; List.head\nlet getFirstUserName = getFirstUser &gt;&gt; getUserName\nlet getUserList () : User list = [] // TODO:  Get users.\n\n// Output\nval getUserName : user:User -&gt; string\nval getFirstUser : users:User list -&gt; User\nval getFirstUserName : (User list -&gt; string)\n</code></pre> <p>Here, we have a <code>getUserName</code> function that takes a <code>User</code> and another function <code>getFirstUser</code> that returns a <code>User</code>.  This allows us to compose a new function <code>getFirstUserName</code> from <code>getFirstUser &gt;&gt; getUserName</code>.  </p> <p>The lego-brick like functionality is very useful for defining more complex functionalities from little building blocks.</p> <p>In the course of developing ASP.NET, I have found that the API usually sits on top of a fairly large codebase.  It is useful to compose more complex functions just before creating API constrollers.  This can be solved by injecting dependencies into controllers using the <code>IControllerActivator</code> interface.  This interface is effectively an override of the default dependency injection system.</p> <p>Here's a sample to get you going with an activator.  I usually put the activator and controllers in the same module, since the controller declares the dependencies and the activator composes those dependencies.</p> <pre><code>[&lt;ApiController&gt;]\n[&lt;Route(\"/api/[controller]\")&gt;]\ntype UserController (getLastCreatedUserName: unit -&gt; string) =\n    inherit ControllerBase()\n\n    [&lt;HttpGet&gt;]\n    member __.Get() = getLastCreatedUserName()\n\ntype MyActivator (config: IConfiguration) = \n    interface IControllerActivator with\n        member __.Create(context: ControllerContext) =\n            let controllerType = context.ActionDescriptor.ControllerTypeInfo.AsType()\n            match controllerType with \n            | c when c = typeof&lt;UserController&gt; -&gt; \n                // Function Composition\n                let getLastCreatedUserName = \n                    getUserList\n                    &gt;&gt; getFirstUser\n                    &gt;&gt; getUserName\n                // Dependency Injection\n                UserController(getLastCreatedUserName) \n                    |&gt; box\n            | _ -&gt; failwith (sprintf \"The controller type, %s, could not be found.\" controllerType)\n</code></pre> <p>Lastly, you will need to register this <code>MyActivator</code> in the <code>ConfigureServices</code> method of <code>Startup</code>. <pre><code>services.AddSingleton&lt;IControllerActivator&gt;(new Activator(configuration))\n</code></pre></p> <p>That's it!  This helps to keep your core library functional while using object oriented techniques for the controllers and startup only.  Very nice!</p>"},{"location":"blog/2019/12/07/docker-sql-server/","title":"SQL SERVER on Docker","text":"<p>Setting up SQL SERVER on a Docker Container is so easy that I cringe at all of the times that I have installed a full-blown SQL SERVER instance on my systems in the past.  </p> <p>What used to take a few hours with several touch points, now just takes a couple of commands and a few mins.</p> <p>Also, since this is on Docker, SQL SERVER is fully isolated from the rest of my system.</p>"},{"location":"blog/2019/12/07/docker-sql-server/#lets-get-this-done","title":"Let's Get This Done","text":"<p>Install the latest with <code>docker pull</code>:</p> <pre><code>docker pull mcr.microsoft.com/mssql/server:2017-latest\n</code></pre> <p>Set the SA password:</p> <pre><code>docker run -e \"ACCEPT_EULA=Y\" -e \"SA_PASSWORD=&lt;YourStrong@Passw0rd&gt;\" `\n   -p 1433:1433 --name sql1 `\n   -d mcr.microsoft.com/mssql/server:2017-latest\n</code></pre> <p>Now, you are free to connect to it with sqlcmd, or you can connect directly from SQL SERVER Management Studio:</p> <p>Be sure to use the <code>hostname</code> as the Server name.</p> <p>This entire setup took 5 mins and is amazingly simple!</p>"},{"location":"blog/2019/12/07/docker-sql-server/#refs","title":"Refs","text":"<ul> <li>https://docs.microsoft.com/en-us/sql/linux/quickstart-install-connect-docker?view=sql-server-2017&amp;pivots=cs1-powershell</li> </ul>"},{"location":"blog/2019/12/07/meta-programming---code-to-write-code/","title":"META Programming - Code to Write Code","text":"<p>As I'm sure any .NET programmer would know, .NET offers a pretty expansive reflection api.  Asking a system to provide reflection information is both a powerful and expensive technique.  It can dramatically reduce the quantity of code, but it comes with the extra overhead in runtime performance.  </p> <p>I have come across situations many times where the code I am writing is very repetitive, and the only way to stop the repetition at runtime is with reflection.  </p>"},{"location":"blog/2019/12/07/meta-programming---code-to-write-code/#validation-example","title":"Validation Example","text":"<p>One common example is validation against table schemas.  If you need to ensure the incoming .NET object will not raise a SQL exception, then you will typically have to check each of the properties on the record per table.  </p> <p>Take the following <code>dbo.visits</code> table:</p> <pre><code>CREATE TABLE dbo.visits (\n    visit_id INT PRIMARY KEY IDENTITY (1, 1),\n    first_name VARCHAR (50) NOT NULL,\n    last_name VARCHAR (50) NOT NULL,\n    visited_at DATETIME,\n    phone VARCHAR(20)\n);\n</code></pre> <p>and corresponding F# record:</p> <pre><code>type Visits =\n    {   visit_id: int\n        first_name: string\n        last_name: string\n        visite_at: DateTime\n        phone: string\n        store_id: int\n        }\n</code></pre> <p>To ensure a successful transaction with this record against the database, then you will need to check:</p> <ol> <li><code>first_name</code> is not null and is 50 characters or less.</li> <li><code>last_name</code> is not null and is 50 characters or less.</li> <li><code>visited_at</code> is greater than the SQL minimum date.</li> <li>...</li> </ol> <p>This might look something like:</p> <pre><code>let validate visit = \n    let errors = \n        [\n            \"first_name\", visit.first_name &lt;&gt; null &amp;&amp; visit.first_name.Length &lt;= 50, \"The first_name is required and must be less than or equal to 50 characters.\"\n            \"last_name\", visit.last_name &lt;&gt; null &amp;&amp; visit.last_name.Length &lt;= 50, \"The last_name is required and must be less than or equal to 50 characters.\"\n            // ...\n        ]\n\n    match errors with \n    | [] -&gt; Ok visit\n    | _ -&gt; Error errors\n</code></pre> <p>Note:  You will need to include unit tests of the validation rules from the table schema.</p> <p>You can see from the <code>validate</code> function above that the pattern of checking each of the fields is very repetitive.</p> <p>'Ain't no body got time for that!'</p>"},{"location":"blog/2019/12/07/meta-programming---code-to-write-code/#keep-it-dry","title":"Keep it D.R.Y.","text":""},{"location":"blog/2019/12/07/meta-programming---code-to-write-code/#information_schema","title":"INFORMATION_SCHEMA","text":"<p>The information that we are extracting from the <code>CREATE TABLE</code> can be found in the <code>INFORMATION_SCHEMA</code> in SQL SERVER.  (Other database providers have their own schema querying views.)</p> <pre><code>SELECT COLUMN_NAME, IS_NULLABLE, DATA_TYPE, CHARACTER_MAXIMUM_LENGTH\nFROM INFORMATION_SCHEMA.COLUMNS\n</code></pre> COLUMN_NAME IS_NULLABLE DATA_TYPE CHARACTER_MAXIMUM_LENGTH visit_id NO int NULL first_name NO varchar 50 last_name NO varchar 50 visited_at YES datetime NULL phone YES varchar 20"},{"location":"blog/2019/12/07/meta-programming---code-to-write-code/#f-schema","title":"F# Schema","text":"<p>Use your standard ORM to query this data out in an <code>fsx</code> script into an F# type:</p> <pre><code>type ColumnSchema =\n    {   COLUMN_NAME: string\n        IS_NULLABLE: string\n        DATA_TYPE: string\n        CHARACTER_MAXIMUM_LENGTH: int option\n        }\n\nlet meta() = \n\n    // TODO: Run query...\n\n    [\n        {   COLUMN_NAME = \"visit_id\"\n            IS_NULLABLE = \"NO\"\n            DATA_TYPE = \"int\"\n            CHARACTER_MAXIMUM_LENGTH = None\n            }\n        {   COLUMN_NAME = \"first_name\"\n            IS_NULLABLE = \"NO\"\n            DATA_TYPE = \"varchar\"\n            CHARACTER_MAXIMUM_LENGTH = Some 50\n            }\n        {   COLUMN_NAME = \"last_name\"\n            IS_NULLABLE = \"NO\"\n            DATA_TYPE = \"varchar\"\n            CHARACTER_MAXIMUM_LENGTH = Some 50\n            }\n        {   COLUMN_NAME = \"visited_at\"\n            IS_NULLABLE = \"YES\"\n            DATA_TYPE = \"datetime\"\n            CHARACTER_MAXIMUM_LENGTH = None\n            }\n        {   COLUMN_NAME = \"phone\"\n            IS_NULLABLE = \"YES\"\n            DATA_TYPE = \"varchar\"\n            CHARACTER_MAXIMUM_LENGTH = Some 20\n            }\n    ]\n</code></pre>"},{"location":"blog/2019/12/07/meta-programming---code-to-write-code/#lets-get-meta","title":"Let's Get META","text":"<p>Convert the repetitive code above into a template.  This is where we treat code as data!</p> <pre><code>let requiredStringTemplate (x: ColumnSchema list = \n    let name = x.COLUMN_NAME\n    let max = x.CHARACTER_MAXIMUM_LENGTH.Value\n    sprintf \"\"\"\n    \"%s\", x.%s &lt;&gt; null &amp;&amp; x.%s.Length &lt;= %i, \"The %s is required and must be less than or equal to %i characters.\"\n    \"\"\" name name name max name max\n\n// TODO:  Provide other rules.\n\nlet template (xs: ColumnSchema list) = \n    let rules = \n        xs \n        |&gt; List.map(fun x -&gt; \n            match (x.DATA_TYPE, x.IS_NULLABLE, x.CHARACTER_MAXIMUM_LENGTH) with \n            | (\"varchar\", \"NO\", Some _) -&gt; requiredStringTemplate x |&gt; Some\n            // TODO:  Call other rules.\n            | _ -&gt; None)\n        |&gt; List.filter Option.isSome\n        |&gt; List.map Option.get\n        |&gt; List.fold (fun a b -&gt; a + \"\\r\\n            \" + b) \"\"\n    \"\"\"\nlet validate x = \n    let errors = \n        [\n            %s\n        ]\n\n    match errors with \n    | [] -&gt; Ok x\n    | _ -&gt; Error errors\n\"\"\" rules\n</code></pre>"},{"location":"blog/2019/12/07/meta-programming---code-to-write-code/#meta-benefits","title":"META Benefits","text":"<ol> <li>META Test - We can now safely delete our unit tests since we would only need to test the META code.</li> <li>Schema Changes - As the schema evolves in the future, you can simply run this job to generate the final validation code.  This ensures that the database schema is the lone, authoritative source of schema information.</li> </ol>"},{"location":"blog/2019/12/07/meta-programming---code-to-write-code/#side-note","title":"Side Note","text":"<p>If you prefer to keep your database schema dumb, then you can move the authoritative source to another format such as <code>yaml</code>.  Then, use that source as your input into the <code>ColumnSchema</code> above.</p> <p>Pretty sweet!</p>"},{"location":"blog/2020/02/24/t-sql-recursive-cte-common-table-expression/","title":"T-SQL Recursive CTE (*Common Table Expression*)","text":"<p>CTEs are useful for modeling queries that operate on recursive structures.</p> <p>I often have the need to categorize or group an entity type with a tree structure.  This requirement can be met with a simple table and CTE to order the tree structure.</p> <p>Let's take the following for example:</p> <pre><code>/*\n    Create a simple `Category` table for storing a \n    tree structure of categories in a flat table.\n\n    Use the `ParentId` to find the parent of the \n    category.\n*/\nCREATE TABLE Category (\n    Id uniqueidentifier NOT NULL\n    , ParentId uniqueidentifier NULL\n    , Title nvarchar(75) NOT NULL\n);\nGO\n\n/*\n    Use a CTE to order the categories\n    by level and parent.\n*/\n;WITH\u00a0Tree(Id,\u00a0ParentId,\u00a0Title,\u00a0[Level])\nAS\u00a0(\n\u00a0\u00a0\u00a0\u00a0SELECT\u00a0Id\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0,\u00a0ParentId\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0,\u00a0Title\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0,\u00a00\u00a0AS\u00a0[Level]\n\u00a0\u00a0\u00a0\u00a0FROM\u00a0Category\u00a0p\n    WHERE ParentId IS NULL\n\u00a0\u00a0\u00a0\u00a0UNION\u00a0ALL\u00a0\n\u00a0\u00a0\u00a0\u00a0SELECT\u00a0t.Id\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0,\u00a0t.ParentId\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0,\u00a0t.Title\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0,\u00a0tree.Level\u00a0+\u00a01\u00a0[Level]\n\u00a0\u00a0\u00a0\u00a0FROM\u00a0Category\u00a0t\n\u00a0\u00a0\u00a0\u00a0JOIN\u00a0Tree\u00a0tree\u00a0ON\u00a0tree.Id\u00a0=\u00a0t.ParentId\n)\nSELECT\u00a0*\u00a0\nFROM\u00a0Tree\nORDER\u00a0BY\u00a0[Level],\u00a0ParentId\n</code></pre>"},{"location":"blog/2020/04/26/haskell-setup/","title":"Haskell Setup","text":"<p>In my journey down the path of purity in functional programming, I have inevitably merged with the path of <code>Haskell</code> programming.  <code>Haskell</code> seems like an amazing language.  I started my career rooted in Mathematics, and I feel at home coding in the <code>Haskell</code> programming language.  </p> <p>But, I've also seen the pain of setting up my dev system for multiple types of coding environments.  For quick console coding, I'd really like to get off the train of managing installs on my system.  </p>"},{"location":"blog/2020/04/26/haskell-setup/#docker","title":"Docker","text":"<p>Let's setup a <code>Haskell</code> dev environment with Docker to isolate the environment.</p> <p>Requirements</p> <ol> <li>The <code>Haskell</code> coding environment should be isolated from the dev system.</li> <li>The *.hs code files should be reloaded without having to rebuild Docker images.</li> </ol> <pre><code># ${PWD} is the current directory in PowerShell\ndocker run -it --rm -v ${PWD}:/app haskell:8\n\n:load app/main.hs\n</code></pre> <p>Let's get to <code>Haskell</code> code!</p>"},{"location":"blog/2020/05/12/use-powershell-to-spin-up-an-integration-test-api/","title":"Use PowerShell to Spin Up an Integration Test API","text":"<p>In developing CI/CD pipelines, I have often come across the need to spin up an api just to run integration tests against.  Below is a PowerShell script I came up with to startup an api on another process, check that it is alive, and then run the integration tests against that api.  </p> <p>Works like a charm!</p> <pre><code># Assume the API is not alive.\n$isAlive\u00a0=\u00a0$false\n\nSet-Location \"path/to/integration/test/api\"\n\n#\u00a0Start Test API\u00a0in the Background\nstart\u00a0dotnet\u00a0run\n\n#\u00a0Wait\u00a0for\u00a0Process\n\n# Define a Check-Process Function\n[Net.ServicePointManager]::SecurityProtocol\u00a0=\u00a0[Net.SecurityProtocolType]::Tls12\nFunction\u00a0Check-Process()\u00a0{\n\u00a0\u00a0\u00a0\u00a0return\u00a0Invoke-RestMethod\u00a0-Method\u00a0get\u00a0`\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-ContentType\u00a0'Application/Json'\u00a0`\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-Uri\u00a0\"https://localhost:5001/health\"\u00a0`\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-ErrorAction\u00a0SilentlyContinue\n}\n\n# Check if the API is alive at most 10 times.\nFor\u00a0($i\u00a0=\u00a00;\u00a0$i\u00a0-le\u00a010;\u00a0$i++)\u00a0{\n\u00a0\u00a0\u00a0\u00a0Write-Output\u00a0\"Sleeping\u00a0for\u00a05\u00a0seconds.\"\n\u00a0\u00a0\u00a0\u00a0Start-Sleep\u00a0-Seconds\u00a05\n\u00a0\u00a0\u00a0\u00a0Write-Output\u00a0\"Trying\u00a0https://localhost:5001/health\"\n\u00a0\u00a0\u00a0\u00a0$isAlive\u00a0=\u00a0Check-Process\u00a0-eq\u00a0\"Healthy\"\n\u00a0\u00a0\u00a0\u00a0if\u00a0($isAlive)\u00a0{\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Break\n\u00a0\u00a0\u00a0\u00a0}\n}\n\n# Stop if the api is not alive at this point.\nif\u00a0(-Not($isAlive))\u00a0{\n\u00a0\u00a0\u00a0\u00a0Throw\u00a0\"The\u00a0Integration\u00a0Src\u00a0API\u00a0is\u00a0not\u00a0alive\u00a0after\u00a050\u00a0seconds.\"\n}\n\nSet-Location \"path/to/integration/tests\"\n\n#\u00a0Run\u00a0Integration Tests\ndotnet\u00a0run\n\n#\u00a0Get\u00a0Process\u00a0Id so that we can kill it later.\n$api_pid\u00a0=\u00a0(Get-NetTCPConnection\u00a0-LocalPort\u00a05001).OwningProcess[0]\n\n#\u00a0Kill\u00a0the API\u00a0Process\nStop-Process\u00a0-Id\u00a0$api_pid\n</code></pre>"},{"location":"blog/2020/05/17/lib-and-coffee/","title":"Lib and Coffee","text":"<p>There's more to software development than code.</p> <p>You have to verify the system works from end to end.  At some point in the dev cycle you will need to plug it all together to make sure it works as expected.  </p> <p>Too much of my career has been focused on ensuring everything works all the time on my system.  As I've grown as a developer, my career has gone through several phases.</p>"},{"location":"blog/2020/05/17/lib-and-coffee/#phase-i-manually-verify","title":"Phase I - Manually Verify","text":"<p>Change a line of code, then manually verify it worked as expected and didn't break anything else.  Keep the code on your system fully operating with each of the following:</p> <ol> <li>Database</li> <li>API</li> <li>Web Services</li> <li>UI</li> </ol> <p>But, a problem quickly arises:  Can you truly verify everyting all the time?</p>"},{"location":"blog/2020/05/17/lib-and-coffee/#phase-ii-automatically-verify","title":"Phase II - Automatically Verify","text":"<p>Enter automated integration testing.  Here's few flavors of integration tests I've written:</p> <ol> <li>SQL Client Database Tests</li> <li>REST API Tests</li> <li>Selenium/E2E Tests</li> <li>Generative Data LOAD Tests</li> <li>PERF Repetition Tests</li> </ol> <p>It never ceases to amaze me how much there is to do with automated testing.</p> <p>These have been great to gain greater confidence in the quality of software.  But, there's a catch.</p> <p>Integration testing by definition is more computationally expensive, time extensive, and generally more involved than UNIT testing.  Putting your integration tests into a CI/CD pipeline also proves to be tricky and can be brittle.  </p>"},{"location":"blog/2020/05/17/lib-and-coffee/#phase-iii-focus-on-the-unit","title":"Phase III - Focus on the UNIT","text":"<p>During the next phase in my career, I turned my attention to UNIT testing.  UNIT testing requires direct inheritance of the production code.  This allows the tests to be decoupled from external concerns such as HTTP connections, database connections, file system access and the like.</p> <p>I can write what seems like 100s of UNIT tests for every integration test.</p> <p>Great!  Now, I can write tests and shorten the feedback loop.  I want to know if I've broken some requirement as soon as possible in the software development cycle.</p> <p>Even here there's a catch though.  External dependencies have to be mocked out.  Through mocking, you can tell the system to do something instead of actually doing the external work.  But, this process has proved to be pain.  The test code is more complicated, takes longer to write, and is harder to maintain.  </p>"},{"location":"blog/2020/05/17/lib-and-coffee/#phase-iv-lib-it","title":"Phase IV - Lib It!","text":"<p>Let me just say that one thing I look forward to everyday is what I call \"Lib and Coffee\".  It's the type of coding that just requires my thoughts, my code, and probably a cup of coffee.  I just have the problem at hand, the knowledge in my mind, and the code on the screen.  </p> <p>I startup watchers that run unit tests on file save.  I write a test failure, write the code, then watch the test turn green.  I can be in this zone for hours on end, and at the end, it feels very satisfying to deliver a quality product that is fully tested.  Very nice!</p> <p>From a functional programming perspective, the <code>Lib</code> is the library where you put your purely functional code.  The functions are transparent in that they declare the names, their input types, and their output types.  External dependencies are handled some where else.   </p> <p>This is the truly zen mode of coding!</p>"},{"location":"blog/2020/05/17/stringbuilder/","title":"StringBuilder","text":"<p>Numerous .NET resources reference <code>StringBuilder</code> as the preferred method for building strings rather than concatentating.  </p> <p>The Microsoft docs describe it this way:</p> <p>\"The String object is immutable. Every time you use one of the methods in the System.String class, you create a new string object in memory, which requires a new allocation of space for that new object. In situations where you need to perform repeated modifications to a string, the overhead associated with creating a new String object can be costly.\"</p> <p>Ref:  https://docs.microsoft.com/en-us/dotnet/standard/base-types/stringbuilder</p> <p>But, just how costly is it?  Let's setup a quick performance test to investigate.  We'll create two functions:  <code>stringAdder</code>, <code>stringBuilder</code>.  Both will ignore the final results since we are investigating performance only.  Also, let's use a simple string of the alphabet.</p> <pre><code>open System\n\n// Simple concatentation\nlet stringAdder : string list -&gt; unit = List.reduce (+) &gt;&gt; ignore\n// Build up the final string with StringBuilder\nlet stringBuilder (xs: string list) = \n    let buf = new Text.StringBuilder()\n    buf.Append xs.Head |&gt; ignore\n    xs.Tail\n    |&gt; List.iter(fun x -&gt; buf.Append x |&gt; ignore)\n    buf \n    |&gt; string\n    |&gt; ignore\nlet genText (folder: string list -&gt; unit) count = \n    let xs = [for _ in 0..count -&gt; \"abcdefghijklmnopqrstuvwxyz\"]\n    xs\n    |&gt; folder\n\n10 |&gt; genText stringAdder\n100 |&gt; genText stringAdder\n1000 |&gt; genText stringAdder\n10000 |&gt; genText stringAdder\n100000 |&gt; genText stringAdder\n\n10 |&gt; genText stringBuilder\n100 |&gt; genText stringBuilder\n1000 |&gt; genText stringBuilder\n10000 |&gt; genText stringBuilder\n100000 |&gt; genText stringBuilder\n1000000 |&gt; genText stringBuilder\n10000000 |&gt; genText stringBuilder\n</code></pre> <p>Wrapping each of working lines above with <code>#time</code> in fsi generates the following stats.</p> Type Count Time Adder 10 00:00:00.002 Adder 100 00:00:00.000 Adder 1,000 00:00:00.020 Adder 10,000 00:00:01.629 Adder 100,000 ***Overflow Builder 10 00:00:00.001 Builder 100 00:00:00.000 Builder 1,000 00:00:00.000 Builder 10,000 00:00:00.001 Builder 100,000 00:00:00.031 Builder 1,000,000 00:00:00.292 Builder 10,000,000 00:00:03.078 Builder 100,000,000 ***Overflow"},{"location":"blog/2020/06/07/duck-typing/","title":"Duck Typing","text":""},{"location":"blog/2020/06/07/duck-typing/#run-time-duck-typing","title":"Run-time Duck Typing","text":"<p>In a dynamically typed language such as <code>JavaScript</code>, you would infer the type of an object by investigating it's members.  Since <code>JavaScript</code> is not a statically typed language, we cannot trust the type information without checking that at run-time.  It is common to see code such as this in <code>JavaScript</code>.</p> <pre><code>if (myObj.Id) {\n    doSomethingWithId(myObj.Id);\n}\n</code></pre> <p>That's great that we can support dynamic member checking, but is there a way that we can have the compiler tell us this ahead of time?</p>"},{"location":"blog/2020/06/07/duck-typing/#net-generics","title":".NET Generics","text":"<p>Coming from <code>C#</code>, I was already comfortable with generic classes and methods.  These provide a template of a code that can be applied to more specific types.  Generics can lead to better code abstraction and reuse.  You can think of generics as contraints upon the types that are used by the generic class.  </p> <p>F# has all of the .NET Generics just like C#.</p>"},{"location":"blog/2020/06/07/duck-typing/#compile-time-duck-typing","title":"Compile-time Duck Typing","text":"<p>But, F# goes a step further.  F# offers static generic constraints.  </p> <p>Whereas C#'s generics are applied to instances of types at run-time, the F#'s static type parameters are resolved by the compiler at compile time!</p> <p>Let's take an example...</p> <pre><code>open System\n\ntype User =\n    { Id : Guid\n      Name : string }\n\nlet inline getId (record: ^T) : Guid = (^T : (member Id : Guid) (record))\n\nlet luke = { Id = Guid.NewGuid(); Name = \"Luke Skywalker\" }\nlet id = getId luke\n</code></pre> <p>Output:</p> <pre><code>type User =\n  { Id: System.Guid\n    Name: string }\nval inline getId :\n  record: ^T -&gt; System.Guid when  ^T : (member get_Id :  ^T -&gt; System.Guid)\nval luke : User = { Id = 802f4714-7ef7-474b-af6b-f3fa05a661f9\n                    Name = \"Luke Skywalker\" }\nval id : Guid = 802f4714-7ef7-474b-af6b-f3fa05a661f9\n</code></pre> <p>At first glance, this may seem like nothing special is going on here.  But, note that I am able to apply the <code>getId</code> function to any type that has an <code>Id</code> of type <code>Guid</code>.  This cuts down the impulse to create type inheritance to share simple data structures and methods among types. </p> <p>I'm able to apply Duck Typing to types and have the compiler tell me as I'm developing if the type is a duck!</p>"},{"location":"blog/2020/07/07/apod-azure-function/","title":"APOD Azure Function","text":"<p>Let's setup a cache of astronomy photos of the day!  </p>"},{"location":"blog/2020/07/07/apod-azure-function/#enter-azure-functions","title":"Enter Azure Functions","text":"<p>I already store my blog on an Azure storage account.  Wouldn't it be great if I could automatically get the APOD photo and put it into that storage account?</p> <p>Since I wrote the original script in JavaScript, I opted for TypeScript as the Azure Function language of choice.  </p> <p>After pulling down an auto-generated Azure Function into VS Code, I did the following.</p> <p>First, I setup the dependencies:</p> <p>package.json:</p> <pre><code>  \"dependencies\": {\n    \"@azure/storage-blob\": \"^12.2.0-preview.1\",\n    \"node-fetch\": \"^2.6.0\"\n  },\n  \"devDependencies\": {\n    \"@azure/functions\": \"^1.0.2-beta2\",\n    \"@types/node-fetch\": \"^2.5.7\",\n    \"typescript\": \"^3.3.3\"\n  }\n</code></pre> <p>Second, I defined the script.  </p> <pre><code>import { AzureFunction, Context } from \"@azure/functions\"\nimport { ContainerClient, StorageSharedKeyCredential } from \"@azure/storage-blob\";\nimport fetch from \"node-fetch\";\n\nconst getFileName = function (url: string) {\n    const s = url.split(\"/\");\n    const l = s.length;\n    return s[l - 1];\n};\n\nconst saveImg = async function (url: string, context: Context) {\n    var timeStamp = new Date().toISOString();\n    context.log('Saving url:  ' + url, timeStamp);\n    const creds = new StorageSharedKeyCredential(\"{STORAGE_ACCOUNT_NAME}\", \"{STORAGE_ACCOUNT_KEY}\");\n    const client = new ContainerClient(\"{STORAGE_ACCOUNT_URI}/assets/img/apod\", creds)\n    const filename = getFileName(url);\n    const blob = client.getBlobClient(filename);\n    await blob.syncCopyFromURL(url);\n};\n\nconst sync = async function (context: Context) {\n    const json_url = \"https://api.nasa.gov/planetary/apod?api_key={APOD_API_KEY}\";\n    context.log(\"Fetching \" + json_url);\n    await fetch(json_url)\n        .then(function (response) { return response.json(); })\n        .then(function (json) {\n            const timeStamp = new Date().toISOString();\n            const img_url = json.url;\n            if (json.media_type !== \"image\") {\n                context.log('The json response is not an image: ' + img_url, timeStamp);\n                return;\n            }\n            context.log(\"Fetched \" + json_url);\n            return saveImg(img_url, context);\n        });\n};\n\nconst timerTrigger: AzureFunction = async function (context: Context, myTimer: any): Promise&lt;void&gt; {\n    const timeStamp = new Date().toISOString();\n    context.log('Syncing', timeStamp);\n    await sync(context);\n};\n\nexport default timerTrigger;\n</code></pre> <p>Third, I set the function to run on a timer since this is the \"Photo of the Day\".  </p> <p>That's it!  Now, I have a simple Azure Function running in the cloud, storing the photos in the cloud, of the astronomy beyond the clouds!  Nice!</p>"},{"location":"blog/2020/08/02/sql-server-on-docker-with-sqlcmd/","title":"SQL SERVER on Docker with SQLCMD","text":"<p>Let's revisit a previous post about using Docker to create a SQL SERVER image.  Previously, I connected to the DB with SQL SERVER Management Studio (SSMS), but now, I'd like to use <code>sqlcmd</code> to avoid running other tools.</p> <p>Let's get the latest mssql image for Linux this time from https://hub.docker.com/_/microsoft-mssql-server.</p> <pre><code># Pull the Image\ndocker pull mcr.microsoft.com/mssql/server:2019-CU5-ubuntu-16.04\n\n# Run the Container\ndocker run -e 'ACCEPT_EULA=Y' -e 'SA_PASSWORD=...' -p 1433:1433 --name sql1 -d mcr.microsoft.com/mssql/server:2019-CU5-ubuntu-16.04\n\n# Start an Interactive Shell\ndocker exec -it sql1 /bin/bash\n\n# Test SQLCMD\nopt/mssql-tools/bin/sqlcmd -?\n</code></pre> <p>Output</p> <pre><code>Microsoft (R) SQL Server Command Line Tool\nVersion 17.5.0002.1 Linux\nCopyright (C) 2017 Microsoft Corporation. All rights reserved.\n\nusage: sqlcmd            [-U login id]          [-P password]\n  [-S server or Dsn if -D is provided]\n  [-H hostname]          [-E trusted connection]\n  [-N Encrypt Connection][-C Trust Server Certificate]\n  [-d use database name] [-l login timeout]     [-t query timeout]\n  [-h headers]           [-s colseparator]      [-w screen width]\n  [-a packetsize]        [-e echo input]        [-I Enable Quoted Identifiers]\n  [-c cmdend]\n  [-q \"cmdline query\"]   [-Q \"cmdline query\" and exit]\n  [-m errorlevel]        [-V severitylevel]     [-W remove trailing spaces]\n  [-u unicode output]    [-r[0|1] msgs to stderr]\n  [-i inputfile]         [-o outputfile]\n  [-k[1|2] remove[replace] control characters]\n  [-y variable length type display width]\n  [-Y fixed length type display width]\n  [-p[1] print statistics[colon format]]\n  [-R use client regional setting]\n  [-K application intent]\n  [-M multisubnet failover]\n  [-b On error batch abort]\n  [-D Dsn flag, indicate -S is Dsn]\n  [-X[1] disable commands, startup script, environment variables [and exit]]\n  [-x disable variable substitution]\n  [-g enable column encryption]\n  [-G use Azure Active Directory for authentication]\n  [-? show syntax summary]\n</code></pre> <pre><code># Run SQLCMD\nopt/mssql-tools/bin/sqlcmd -S localhost -U sa -P Password12345 -Q \"SELECT * FROM INFORMATION_SCHEMA.TABLES\"\n</code></pre> TABLE_CATALOG TABLE_SCHEMA TABLE_NAME TABLE_TYPE master dbo spt_fallback_db BASE TABLE master dbo spt_fallback_dev BASE TABLE master dbo spt_fallback_usg BASE TABLE master dbo spt_values VIEW master dbo spt_monitor BASE TABLE master dbo MSreplication_options BASE TABLE <p>Very Nice!</p>"},{"location":"blog/2020/09/12/avoid-mockingstubbing-services/","title":"Avoid Mocking/Stubbing Services","text":"<p>Let's say that we have a function that calls an external service:</p> <pre><code>let sayHelloToCurrentUser () = \n    // Call External Service\n    AccountService.getUsername()\n    |&gt; sprintf \"Hello %s!\"\n\n// Integration Test\nlet [&lt;Fact&gt;] ``should say Hello to the name``() =\n    sayHelloToCurrentUser()\n    |&gt; equal \"Hello Joel!\"\n</code></pre> <p>That may look simple enough, except that this function is technically an integration function since it makes an external call.  It is difficult to guarantee the service availability and connections in a CI (continuous integration) environment.  </p> <p>It is best to isolate the external calls in UNIT tests so that the run time only has to have the code itself to run the tests.</p> <p>There are two primary options you can use to isolate the external call.</p> <ol> <li>Put the service call behind an interface and provide fake implementations with mocking or stubbing.</li> <li>Isolate the \"testable\" functions from the service connections.</li> </ol> <p>For the first approach, we can provide a callback function to <code>getUsername</code>.</p> <pre><code>let sayHelloToCurrentUser (getUsername: unit -&gt; string) = \n    getUsername()\n    |&gt; sprintf \"Hello %s!\"\n\n// UNIT Test\nlet [&lt;Fact&gt;] ``should say Hello to the name``() =\n    // Function Stub\n    fun _ -&gt; \"Joel\"\n    |&gt; sayHelloToCurrentUser\n    |&gt; equal \"Hello Joel!\"\n</code></pre> <p>The complexity of stubbing a service can grow rather quickly, though.  You can probably imagine that the AccountService has lots of methods exposed.  We would end up putting the service behind a .NET interface so that we can provide a fake implementation in our UNIT tests.  </p> <p>But, there's a simpler way.  </p> <pre><code>// Lib\nlet sayHello name = sprintf \"Hello %s!\" name\n\n// Composition\nlet sayHelloToCurrentUser = AccountService.getUsername &gt;&gt; sayHello\n\n// UNIT Test\nlet [&lt;Fact&gt;] ``should say Hello to the name``() =\n    \"Joel\"\n    |&gt; sayHello\n    |&gt; equal \"Hello Joel!\"\n</code></pre> <p>Now, we are simply isolating the integration point to a module devoted to composition.  </p> <p>Now, we just have a simple function in our <code>Lib</code>.  It doesn't require any stubbing or mocking.  Very nice indeed!</p>"},{"location":"blog/2021/03/31/git-release-notes/","title":"Git Release Notes","text":"<p>The problem often arises in software engineering to capture release notes for your end users.  The situation quickly complicates because the notion of \"end users\" changes per environment.  The developers may be using a DEV environment while the QA may be using another.  And, then you may need another UAT environment with the primary stake holders.  Each of these environments should have a some form of documentation for the release.  </p> <p>Enter <code>git</code> to the rescue.  I typically follow the 'git flow' workflow.  Also, over time I have begun to use <code>rebase</code> with <code>squash</code> instead of a simple <code>merge</code> to keep the commit log clean.  With this workflow, it is very easy to setup a simple CI job that logs the changes from git since the last commit.  </p> <p>The script below hinges on the following git command:</p> <pre><code>git diff --name-only HEAD~0 HEAD~1\n</code></pre> <p>Here's a <code>pwsh</code> script to get you started.  Enjoy!</p> <pre><code>Write-Host \"Running Release Notes\" -ForegroundColor Yellow\n\n\nFunction Save-CommitNotes([int]$commitIndex) {\n\n    # Add the Release Notes\n    $releaseNotes = \"# Release Notes`n`n\"\n    $commits = git log -200 --pretty='%H'\n    $commitHash = $commits[$commitIndex]\n    $commitTitle = git log -1 --pretty='%s' $commitHash\n    $date = git show --no-patch --no-notes --pretty='%cd' $commitHash\n\n    Write-Host \"Logging $commitHash from $date at index $commitIndex.\" -ForegroundColor Yellow\n\n\n    # Add the commit message and hash title.\n    $commitUrl = git remote get-url origin | Foreach-Object { $_ -replace \"\\.git\", \"/-/commit/$commitHash\" }\n    $releaseNotes += \"### [$commitTitle]($commitUrl)`n`n\"\n\n\n    $releaseNotes += \"&gt; $date`n`n\"\n\n\n    # Get the file changes in the last commit\n    $nextCommitIndex = $commitIndex + 1\n    # https://git-scm.com/docs/git-diff#Documentation/git-diff.txt---diff-filterACDMRTUXB82308203\n    git diff HEAD~$nextCommitIndex HEAD~$commitIndex --name-status |\n    ForEach-Object {\n        $line = $_ -replace \"`t\", \" \"\n        # Remove the Status code and leading space\n        #   Example:  'M       .gitignore'\n        $title = $line.Substring(1, $line.Length - 1).Trim()\n        $status = $line[0]\n\n\n        switch($status) {\n            # Added\n            \"A\" { $title = \"\u2795 $title\" }\n            # Modified\n            \"M\" { $title = \"\ud83d\udd8a $title\" }\n            # Renamed\n            \"R\" { $title = \"\u21aa $title\" }\n            # Deleted\n            \"D\" { $title = \"\u2796 ~~$title~~\" }\n            # Other\n            default { $title = \"($status) $title\"}\n        }\n\n        if ($line.EndsWith(\".md\") -and ($status -ne \"D\")) {\n            # Convert the markdown file changes to links.\n            # Set the url as the last segment after the space\n            $split = $line.Split(\" \")\n            $url = $split[$split.Length - 1]\n            $releaseNotes += \"- [$title]($url)`n\"\n        }\n        else {\n            $releaseNotes += \"- $title`n\"\n        }\n    }\n\n\n    if (Test-Path(\"index.md\")) {\n        $releaseNotes = @($releaseNotes) + (Get-Content index.md | Select-Object -Skip 1)\n    }\n\n\n    $releaseNotes | Set-Content index.md\n}\n\n\n### Log Previous Commits ###\n\n\n# Find the last saved commit if it exists.\nif (Test-Path .\\release.data) {\n    $lastCommit = (Get-Content .\\release.data).Substring(0, 40)\n    $commits = git log -200 --pretty='%H'\n    $foundIndex = [array]::indexof($commits, $lastCommit)\n    Write-Host \"Release Data Found:  $lastCommit at index $foundIndex\"\n    if ($foundIndex -gt 0) {\n        ($foundIndex-1)..0 | ForEach-Object { Save-CommitNotes $_ }\n    } else {\n        Write-Host \"There are no previous commits to log.\" -ForegroundColor Yellow\n    }\n} else {\n    # Default the search to 20 previous commits.\n    20..0 | ForEach-Object { Save-CommitNotes $_ }\n}\n\n\n\n# Save the Previous Commit\ngit log -1 --pretty='%H' | Set-Content .\\release.data\n</code></pre>"},{"location":"blog/2021/04/05/decorators/","title":"Decorators","text":""},{"location":"blog/2021/04/05/decorators/#python-decorators","title":"Python Decorators","text":"<p>I find the Python notion of a decorator intriguing.  Let's take an example:</p> <pre><code>import functools\n\ndef logit(func):\n    \"\"\"Prints messages when entering and leaving the decorated function\"\"\"    \n    @functools.wraps(func)    \n    def wrapper(*args, **kwargs):\n        print(\"Entering: \", func.__name__)\n        result = func(*args, **kwargs)\n        print(\"Result: \", result)\n        print(\"Exiting: \", func.__name__)\n        return result\n    return wrapper\n\n@logit\ndef adder(a, b):\n    return a + b\n\ndef main():\n    returned_value = adder(1, 2)\n    print(\"Returned Value: \" + str(returned_value))\n\nif __name__ == '__main__':\n    main()\n</code></pre> <p>Output</p> <pre><code>Entering:  adder\nResult:  3\nExiting:  adder\nReturned Value: 3\n</code></pre> <p>In the code above, a decorator function, <code>logit</code>, takes a function, <code>func</code>, and wraps it with the <code>wrapper</code> logic. Any function then can be decorated with the <code>@logit</code> decorator.   </p>"},{"location":"blog/2021/04/05/decorators/#net-attributes","title":".NET Attributes","text":"<p>.NET on the other hand provides attributes that you can markup your classes, methods, properties, etc. with extra logic.  Here's an example of an attribute in C#:</p> <pre><code>[DoSomething]\npublic class MyClass\n{\n\n}\n</code></pre> <p>The <code>[DoSomething]</code> attribute in C# can be used to provide extra functionality on the <code>MyClass</code> type.  Though this feature of .NET does provide decoration of a class, it is not quite as flexible as the Python version.  The most notable differences are the <code>*args</code> and <code>**kwargs</code> that provide variable arguments and key-word arguments into the wrapper function that are spread to the decorated function.  C# doesn't have a notion of spreading variable arguments onto a function call (unless you drop down to obscure .NET reflection).</p>"},{"location":"blog/2021/04/05/decorators/#functional-decorators","title":"Functional Decorators","text":"<p>Is there a way to implement a functional style of decoration in .NET similar to Python?  Let's take a crack at this with F#:</p> <pre><code>open System.Diagnostics\n\n// Create a performance timer decorator function.\nlet performanceTimer func = \n    let wrapper x = \n        let timer = new Stopwatch()\n        printfn \"Starting Timer for Parameter:  %A\" x\n        let result = func x\n        timer.Stop()\n        printfn \"Stopped:  %A\" timer.Elapsed\n        result\n    wrapper\n\n// Create the function to decorate.\nlet getSum = fun x -&gt; [0..x] |&gt; List.reduce (+)\n\n// Create the decorated function.\nlet logAdder = getSum |&gt; performanceTimer\n\n// Test the decorated function.\nlogAdder (100000000)\n</code></pre> <p>Output</p> <pre><code>Starting Timer for Parameter:  100000000\nStopped:  00:00:00\nval performanceTimer : func:('a -&gt; 'b) -&gt; ('a -&gt; 'b)\nval getSum : x:int -&gt; int\nval logAdder : (int -&gt; int)\nval it : int = 987459712\n</code></pre> <p>Very nice!  F# allows computing new functions from functions just like in Python.  Since every function in F# takes exactly one parameter (taking into account partial application and tuples as a single parameter), the need of a spread operator is avoided.  Note that F# infers the type of the <code>performanceTimer</code> to be <code>func:('a -&gt; 'b) -&gt; ('a -&gt; 'b)</code> which is properly generic and is only later specified to be about integers.  </p> <p>Also, an added benefit of this construction is that the final, decorated function, <code>logAdder</code>, has the same type, <code>int -&gt; int</code>, as the function to decorate, <code>getSum</code>.</p>"},{"location":"blog/2021/06/08/functional-dependency-injection/","title":"Functional *Dependency Injection*","text":"<p>If you've been developing for at least a year or two with an object oriented language, then you probably have come across the idea of Dependency Injection.  Dependency Injection is useful to isolate dependencies and provide loose coupling of components.  </p> <p>Typically, in C# you define interfaces that provide the required dependencies and constructors (and properties) that require those dependencies.  Then, you would implement an IoC container that does the work to construct the actual dependencies at runtime.  </p> <p>Dependency Injection is great for handling external connections since testing code that calls external connections without injection requires integration.</p> <p>I remember struggling with the idea of Dependency Injection for a few years.  I understood the mechanics of what I needed to do to get the system to work, but I didn't really understand why I needed to do them.  </p> <p>I then began to break apart the phrase.  What qualifies as a \"dependency\"?  What does it mean to inject them?  What's the smallest possible interface of a dependency?  </p> <p>Later, through my study of F#, I came across the idea that a function is the smallest possible interface.  It defines a name, some input, and some output.  That's it.  </p> <p>Aren't all parameters dependencies?  In one sense, yes, but in context to Dependency Injection, no.  The focus on Dependency Injection is on separating the construction of objects from the behavior.  Functions that require parameters may not be created by the client directly.  </p> <p>But, to setup functional Dependency Injection with F#, you can put your dependencies behind a computed function.</p> <p>Let's take a real world example.</p> <pre><code>type HttpResult = ... // Type to capture the results of an HttpResponseMessage\n\n/// Encapsulates the HttpClient SendAsync \nlet request : HttpRequestMessage -&gt; Async&lt;HttpResult&gt; = \n    fun (req: HttpRequestMessage) -&gt; \n        async {\n            // Handle the lifetime of the client.\n            use client = new HttpClient()\n            // Await the response.\n            let! response = client.SendAsync(req) |&gt; Async.AwaitTask\n            // Map the repsonse to a result \n            // since the client will soon be disposed.\n            return response |&gt; mapToHttpResult\n        }\n\nmodule Lib =\n    /// Fetch the user by name with the `request` function.\n    let fetchUserByName (request: HttpRequestMessage -&gt; Async&lt;HttpResult&gt;) (name: string) : User option = \n        // Use `request` to call web service.\n        ...\n\n/// Get the user by name.\nlet getUser : string -&gt; User option = Lib.fetchUserByName request\n</code></pre> <p>The critical dependency of <code>Lib.fetchUserByName</code> is required first:  <code>request: HttpRequestMessage -&gt; Async&lt;HttpResult&gt;</code>.  This function will likely make an actual web request and has a pretty hefty, disposable object, <code>HttpClient</code>, backing it.  </p> <p>The function, <code>Lib.fetchUserByName</code>, doesn't care about how the http request is made or how the connections and objects are disposed. The <code>getUser</code> also shouldn't care about how to make an http request or even how.  We can now use <code>getUser</code> like this:</p> <pre><code>let johnOption : User option = getUser \"John\"\n</code></pre> <p>But, probably the coolest part of all this is that you can swap out dependencies in computing the <code>getUser</code> function as long as the final computed function resolves to <code>string -&gt; User option</code>.  You could, for example, get the user from a SQL database or a NoSQL data store, or whatever!</p>"},{"location":"blog/2021/06/08/git-flow/","title":"Git Flow","text":"<p>First off, you're doing great! You are doing your best to keep track of the changes to your system.  The people on your team know what's going on ... usually.  Code conflicts only turn into actual conflicts sporadically.  But, hey, that's just part of the game, isn't it?  You only run <code>git blame</code> sometimes - no, not to shame anyone, just blame.  </p> <p>But, we can do better!</p> <p>Git is a powerful (if not all-consuming) source control system.  I've been using it for a number of years now, and my particular workflow with Git has changed pretty dramatically over time.  </p>"},{"location":"blog/2021/06/08/git-flow/#git-push-pull-flow","title":"Git Push-Pull Flow","text":"<p>In my early days of learning and using Git, I opted for the simple push-pull workflow.  The workflow was simply:</p> <ol> <li>Fetch other developers' code.</li> <li>Merge the changes.</li> <li>Resolve any conflicts.</li> <li>Commit the changes.</li> <li>Push the new commit.</li> </ol> <p>If there are no conflicts, then the above workflow shortens down to:</p> <ol> <li>Fetch other developers' code.</li> <li>Merge the changes with fast-forward. (No new commit!)</li> </ol> <p>This works fine for small systems but begins to have difficulties with larger systems.  Systems grow in complexity in a number of ways, including:  </p> <ul> <li>number of developers</li> <li>number of existing features</li> <li>number of features in development</li> <li>number of distributed sub-systems</li> <li>number of languages and frameworks</li> <li>number of supported environments </li> <li>different users per environment </li> <li>quantity of code</li> <li>etc.</li> </ul>"},{"location":"blog/2021/06/08/git-flow/#noisy-commits","title":"Noisy Commits","text":"<p>The difficulty comes from providing a meaningful commit log of the changes over time in complex systems.</p>"},{"location":"blog/2021/06/08/git-flow/#feature-branch-commits","title":"Feature Branch Commits","text":"<p>Developers should be branching their code while working through a feature.  And, they should be trying things out in that branch.  This can and should lead to learning moments.  Learning means the developer found a better way.  Commit after commit may say something like \"Changed this thing\" and \"Undid my last change because of this reason\".  </p> <p>Anger-Danger:  Coming across the previous commit may lead to unexpected conclusions about the current state of the system. What do you mean you \"Changed this thing\"?  </p>"},{"location":"blog/2021/06/08/git-flow/#merge-commits","title":"Merge Commits","text":"<p>Probably a little less egregious but nonetheless annoying is the \"Merge Commit\" commit (ironic redundancy?).  It's likely that most developers would agree that a commit titled, \"Merge Commit\" is generally not helpful.  So what?  You merged the branches.  What did you actually do to the system?</p>"},{"location":"blog/2021/06/08/git-flow/#cleanup-commits","title":"\"CLEANUP\" Commits","text":"<p>Should developers be cleaning their code?  Of course they should!  How helpful is a commit that says something like, \"Removed unnecessary whitespace; provided better variable names; did such and such and yada yada.\"</p> <p>Ha!  I even went a step further and waited for a valid commit message to commit cleanup changes.  Garbage!</p>"},{"location":"blog/2021/06/08/git-flow/#squash-rebase-flow","title":"Squash-Rebase Flow","text":"<p>So, how do we get rid of those pesky commits?  Well, squash them or rebase your work on top of another developer's work.  Granted both ideas may seem a little less intuitive at first from Push-Pull, but hang in there.  </p>"},{"location":"blog/2021/06/08/git-flow/#squash-commit","title":"Squash Commit","text":"<p>Squashing your commits is to create a new commit with the net-result of several commits.  This effectively throws away the intermediate commit messages and the details about the specific changes to that commit.</p>"},{"location":"blog/2021/06/08/git-flow/#reset-commit","title":"Reset Commit","text":"<p>A corollary to a squash commit is to reset (<code>git reset --soft</code>) a current branch to a commit.  If you want to clean several commits before pushing, then you can reset to the commit just before the last one you want to remove.  Then, you can commit all of the changes that are left in your repository.</p>"},{"location":"blog/2021/06/08/git-flow/#rebase-commit","title":"Rebase Commit","text":"<p>It took me awhile to get my mind around rebase.  This probably stemmed from my lack of use of the word itself.  Instead of merging two branches, you can treat the other branch as the new base of your current branch.  Git will take each of your commits beyond the split of the two branches and reapply them to the rebased branch.  </p> <p>That's it!  Now, you can create clean and meaningful commit logs that you can use for debugging, blaming (shame-shame), or release tracking.  Get rid of those pesky commits!</p>"},{"location":"blog/2021/06/08/git-flow/#ps-beyond-git","title":"P.S. Beyond Git","text":"<ul> <li>Now enter pull request, merge request, and the like.  Systems such as Github and GitLab provide tooling to the heavy lifting for squashing and rebasing for you while also providing the ability to capture comments and other details to request.  </li> </ul>"},{"location":"blog/2021/09/06/f-vs-ts-vs-js/","title":"F# vs TS vs JS","text":"<p>At first glance, you may wonder how F# relates to the TS vs JS debate, but the idea of transpiling from one language to another extends much further than TS to JS.  </p> <p>TL;DR - TS doesn't go far enough in providing tooling to a pre-transpiled JS language.  F# with the Fable compiler goes the necessary distance with type safety, minimal boilerplate, and awesome type inference to boot!  (imo)</p> <p>For frontend development, JS is the runtime language.  There isn't really a debate about this because of its ubiquity.  JS is certainly not a perfect language (whatever that means) since it has its quirks and its own boilerplate just like most other languages.  </p>"},{"location":"blog/2021/09/06/f-vs-ts-vs-js/#quirkiness-and-boilerplate","title":"Quirkiness and Boilerplate","text":"<p>Sure, F# has some of its own quirks, but its boilerplate is significantly reduced.  Also, I've found my thought-experience in F# is much more even and metrical with a step-by-step flow through development.  The mechanics of both TS and JS, on the other hand, consistently feel like an interruption to my thinking.</p> <p>My thinking in F# is \"I'd like this, and this, and ...\" while with TS its \"do this, oops, I forgot..also do this...and oh ya...do this.\"</p> <p>It appears to me that TS added significant overhead in boilerplate as compared to JS.  Some of the features of TS have become down-right quirky in trying to strongly type a dynamic language.  For example, inferring the keys of an object literal in a switch statement is just straight-up bizarre!</p>"},{"location":"blog/2021/09/06/f-vs-ts-vs-js/#type-inference","title":"Type Inference","text":"<p>In TS, I find that I am spending more time explicitly telling the compiler what I want rather than thinking at the higher, more abstract level.  The true goal of abstraction is to improve the semantics of code.  I'm not interested in a language that keeps me from true abstractions.  With improved semantics our language shifts from telling the computer what we want to describing what we want.</p> <p>My F# experience has clouded my perspective with TS here.  The F# type inference is absolutely amazing.  F# will most of the time just figure things out without you having to specifically tell it.  My attention in F# is on the semantic abstraction.</p> <p>Of course, there's nothing quite like type inference in JS other than your IDE trying to provide some suggestions.  Or, there is the more aggressive type coercion!</p>"},{"location":"blog/2021/09/06/f-vs-ts-vs-js/#type-safety","title":"Type Safety","text":"<p>TS is nice to enforce type safety.  But, the longer I've developed the more that I realize that this idea has come to be quite elusive.  Sure, you get some assurances by the compiler telling you when you've made a syntactic error or when the types don't line up in some way. But, I have to admit that I have rarely (if ever) let a syntactic or type error escape development in JS (or T-SQL ... if that counts \ud83d\ude42).  </p> <p>My gravitation towards type safety has honestly been to support any developer who may be put on my team regardless of their skill level and attention to detail.  I now realize that this is flawed thinking.  If a developer doesn't have the attention to detail or isn't skilled enough, then I don't want them on my team.  And, that's that.</p> <p>Type safety is not inherently bad, but it should not come at the cost of verbosity.  </p> <p>Again, here's where F# wins.  The compiler is even stricter than TS in that it enforces immutability by default and through type inference it doesn't come with the extra penalty of verbosity.  </p>"},{"location":"blog/2021/09/06/f-vs-ts-vs-js/#the-best","title":"The Best!","text":"<p>By taking quirkiness, boilerplate, type inference, and type safety into account, F# is the clear winner in my mind.  </p> <p>But the story doesn't end there.  There's more to consider with software engineering than the simplicity and expressiveness of the development language.  I have to consider how others will maintain and contribute to the code that I write.  I have to ask larger questions about team dynamics, skill, and openness to change.</p> <p>I've recently found myself surrounded by Python developers.  I'm coming from a full-stack .NET team to a tech stack built upon Python.  Python is an interesting language to me with its foundation in academia and its start in Mathematics.  And, it's dynamic (enter \ud83d\ude31)!  </p> <p>Taking this into consideration, which language should I use to code the frontend?  </p> <p>Honestly, I think I'll just drop down to plain-old JavaScript.  If I can't go the extra step to get the type inference and expressivity of F#, then I'd prefer to avoid TS altogether.  I'm not completely against TS \u2500 I'll just wait for the right project to come along before I use it.</p>"},{"location":"blog/2021/12/03/git-flow-simplified/","title":"Git Flow Simplified","text":"<p>Oh, there are so many ways of managing git repositories.  I've written on this topic before, but as usual, things have changed.  I recently changed teams at work, and with a new team comes a new culture of devops.  </p>"},{"location":"blog/2021/12/03/git-flow-simplified/#tldr","title":"TL;DR","text":"<p>\"CI\" means to continuously integrate, so sync often.</p>"},{"location":"blog/2021/12/03/git-flow-simplified/#simple-git-flow","title":"Simple Git Flow","text":"<ol> <li>Keep to a single trunk (such as <code>/develop</code>)</li> <li>Commit on all logically complete units of code.</li> <li>Fetch from remote.</li> <li>Rebase.</li> <li>Push.</li> <li>Repeat &gt; 5x/day</li> </ol> <p>I usually script the above like this:</p> <p>sync.sh</p> <pre><code>#!/bin/sh\nmessage=$1\ngit add -A\ngit commit -m \"${message}\"\ngit fetch\ngit rebase\ngit push\n</code></pre>"},{"location":"blog/2021/12/03/git-flow-simplified/#ci-vs-branching","title":"CI vs Branching","text":"<p>By definition \"continuous integration\" and \"branching\" are antithetical to each other.  I used to think of \"CI\" as something the server does to integrate developers code.  But, I now see that the developers' systems are also a point of integration.  </p> <p>The goal is to integrate as quickly as possible (hence \"continuous\").  </p>"},{"location":"blog/2021/12/03/git-flow-simplified/#still-branch","title":"Still Branch?","text":"<p>Ok, ok, I have to be pragmatic and say that I still branch.  I now use branching for experimental coding, prototyping, and just figuring things out.  But I try to get off the branch within a couple of hours.  </p> <p>Quite frankily, if I knew everything about all code that I'll ever need to write, then I would avoid branching completely.  </p> <p>So, for branching, I usually script it such as this:</p> <p>branch.sh</p> <pre><code>#!/bin/sh\nbranch=$1\ngit checkout -b $branch\ngit push -u origin $branch\n</code></pre> <p>Then, when I'm ready to merge back to trunk (<code>/develop</code>) I complete my branch:</p> <p>complete.sh</p> <pre><code>#!/bin/sh\nmessage=$1\ngit add -A\ngit commit -m \"${message}\"\ngit push\ncurrent_branch=$(git branch --show-current)\ngit checkout develop\ngit merge --squash $current_branch\ngit commit -m \"${message}\"\ngit push $1\ngit branch -d $current_branch\ngit push origin --delete $current_branch\n</code></pre> <p>Note: I use <code>git merge --squash $current_branch</code> to keep the commits on trunk (<code>/develop</code>) clean.</p>"},{"location":"blog/2021/12/07/docker-scratchpad/","title":"Docker Scratchpad","text":"<p>Docker is great for production use, but have you considered using it to quickly take out tools for a spin?  I find I'm reaching for the following command multiple times throughout the day.</p> <p><code>docker run -it --rm --entrypoint /bin/sh $TOOL</code></p> <p><code>$TOOL</code> can be whatever linux based tool I'm interested in trying out. Let's take out <code>alpine</code> linux for a spin.</p> <p><code>docker run -it --rm --entrypoint /bin/sh alpine</code></p> <p></p> <p>Or, I might spin up a <code>python</code> instance to take a look around Python.</p> <p><code>docker run -it --rm --entrypoint /bin/sh python</code></p> <p></p> <p>It's also great for other languages you might want to take out for a spin, such as:</p> <p><code>docker run -it --rm --entrypoint /bin/sh haskell</code></p> <p></p> <p>Pretty cool, eh?</p>"},{"location":"blog/2023/05/25/python-annotations/","title":"Python Annotations","text":"<p>Python is great language to prototype ideas.  The dynamic runtime opens up some possibilities that are very difficult in statically typed languages.  </p> <p>However, there's a catch:  the dynamic runtime should not come at the cost of poor code documentation.  Let's take this simple, arbitrary function:</p> <pre><code>def transform(data):\n    '''Transform this data.'''\n\n    return [\n        item * idx\n        for idx, item in enumerate(data)\n    ]\n</code></pre> <p>The <code>transform</code> function above is pretty straightforward.  It multiplies the numbers in the list by their index, right?  Let's take it out for a spin:</p> <pre><code>print(transform([1, 2, 3]))\n# [0, 2, 6]\n</code></pre> <p>Yep!  That's simple.  But, wait, what about this?</p> <pre><code>print(transform([['a'], ['b'], ['c']]))\n[[], ['b'], ['c', 'c']]\n</code></pre> <p>What's going on here?  Well, nothing in the function said to explicitly work with integers.  How would you restrict the types of inputs so that you can reason about the behavior more simply?  </p> <p>Also, there's another problem:  the function signature doesn't inform you of it's behavior.  You have to read the function implementation to know what it does.  The function signature is the function name, it's arguments' names and types, and the return type.  We should be able to read the signature <code>def transform(data)</code> and know exactly what's going on.  This problem cannot be over-stated.</p> <p>Enter Python type annotations!  </p> <pre><code>def multiply_by_index(data: list[int]) -&gt; list[int]:\n    '''Multiply the numbers by their index.'''\n\n    return [\n        num * idx\n        for idx, num in enumerate(data)\n    ]\n</code></pre> <p>Now, the function signature is very informative:  <code>def multiply_by_index(data: list[int]) -&gt; list[int]</code>.  And, the types are restricted at code-time:</p> <p></p> <p>Note that this restriction only applies while coding; the runtime still behaves the same:</p> <pre><code>print(multiple_by_index([1, 2, 3]))\nprint(multiple_by_index([['a'], ['b'], ['c']]))\n# [0, 2, 6]\n# [[], ['b'], ['c', 'c']]\n</code></pre> <p>The runtime/code-time distinction might seem unhelpful, but I have experienced first-hand seeing the benefits of annotations in large codebases.  The clearer function signatures allow me to build up ever more complicated logic while ignoring the implementation details of the simpler functions.</p>"},{"location":"blog/2023/06/15/dockerize-your-development-today/","title":"Dockerize Your Development Today!","text":""},{"location":"blog/2023/06/15/dockerize-your-development-today/#no-more-it-works-on-my-machine-excuses","title":"No More \"It Works on My Machine\" Excuses","text":"<p>We've all been there. Code that works on my machine doesn't work on yours and vice versa. But, why?  There are like a number of issues to troubleshoot.  Some of the most common are:</p> <ol> <li>Different versions of the programming language, sdk, and runtime.</li> <li>Different package versions.</li> <li>Different operating systems.</li> <li>Different software toolchains.</li> <li>...</li> </ol> <p>The list goes on and on.</p>"},{"location":"blog/2023/06/15/dockerize-your-development-today/#docker-to-the-rescue","title":"Docker to the Rescue","text":"<p>Docker is great for containerizing your services in production.  But, we can do better.  Why not use it to containerize your development environment?  This way, you can be sure that your development environment matches your production environment exactly.  No more surprises, no more excuses.</p> <p>You can simply build a new image when anything on the list above changes and push it to shared container registry for the team.  Then, you can check in the image tag to version control and everyone can spin up an identical environment in minutes.  No more wasted time setting up dependencies and configuring tools.</p> <p>Check out vscode's dev containers for more information.</p> <p>Once I started using docker for development, I never looked back.  It's a game changer.</p>"},{"location":"blog/2023/12/10/use-a-run-script/","title":"Use a Run Script","text":""},{"location":"blog/2023/12/10/use-a-run-script/#why-run","title":"Why <code>run</code>?","text":"<p>This <code>run</code> script was inspired by <code>npm run</code> scripts.  Moving your development flow to a run script allows you to setup environments, install, or anything else you need to develop your project.  The <code>run</code> script also allows you to consolidate your scripts into a single script.  You'd likely have an <code>install.sh</code> script, another for <code>test.sh</code>, etc.</p>"},{"location":"blog/2023/12/10/use-a-run-script/#how-to-run","title":"How to <code>run</code>?","text":""},{"location":"blog/2023/12/10/use-a-run-script/#add-current-directory-to-path","title":"Add Current Directory to <code>$PATH</code>","text":"<p>To start, add the current directory to your <code>$PATH</code>.  I prefer to run scripts by name rather than with the relative path prefix (<code>./</code>).  So, I usually use <code>run</code> rather than <code>./run</code>.</p> <pre><code># Add to `~/.bashrc`\nexport PATH=\"$PATH:.\"\n</code></pre>"},{"location":"blog/2023/12/10/use-a-run-script/#add-the-script","title":"Add the Script","text":"<p>Setup a run script for quick development.</p> <p>Create the run script file:</p> <pre><code>$ touch run\n$ chmod +x run\n</code></pre> <p>Add the run script:</p> <pre><code>#!/bin/bash\n\nhelp() {\n    echo \"App\"\n    echo \"=========================\"\n    echo \"start     Run the application\"\n    echo \"test      Test the application\"\n}\n\n# Setup Tab Complete\n_tab_complete() {\n    local cur prev opts\n    opts=\"start test\"\n    COMPREPLY=()\n    cur=\"${COMP_WORDS[COMP_CWORD]}\"\n    prev=\"${COMP_WORDS[COMP_CWORD-1]}\"\n    COMPREPLY=( $(compgen -W \"${opts}\" -- ${cur}) )\n    return 0\n}\n\ncomplete -F _tab_complete run\n\ncmd=$1\n\n# Remove the first argument (shift left)\nshift\n\ncase $cmd in\n    \"start\")\n        echo \"TODO: Add [start]\"\n        # Use \"$@\" to pass all remaining arguments.\n        ;;\n    \"test\")\n        echo \"TODO: Add [test]\"\n        # Use \"$@\" to pass all remaining arguments.\n        ;;\n    *)\n        help\n        ;;\nesac\n</code></pre> <p>Example run:</p> <pre><code>$ . run # Source the run script to add tab complete.\nApp\n=========================\nstart     Run the application\ntest      Test the application\n$ run start\nTODO: Add [start]\n</code></pre>"},{"location":"blog/2024/01/08/development-integration/","title":"Development Integration","text":""},{"location":"blog/2024/01/08/development-integration/#problem","title":"Problem","text":"<p>When developing microservices, we often run into the problem of needing other services to be available to test our application.  Our development flow should have different levels of integration.</p>"},{"location":"blog/2024/01/08/development-integration/#levels-of-integration","title":"Levels of Integration","text":"<ol> <li>Isolate: Develop the service in isolation from other services.</li> <li>Relate: Develop the service alongside other, highly related services.</li> <li>Complete: Deploy the service to a test environment with the full suite of services available.</li> </ol>"},{"location":"blog/2024/01/08/development-integration/#complete","title":"Complete","text":"<p>It is tempting to develop with the complete suite of services, but this doesn't scale well.  Developer systems cannot handle the full scope of distributed systems in a cloud native environment.  We should only strive to have remote test environments include the full suite of services available.</p>"},{"location":"blog/2024/01/08/development-integration/#relate","title":"Relate","text":"<p>It is critical that we develop the related services together.  For instance, we may have the following services closely related to a business domain:</p> <ol> <li>Nginx Static File Server</li> <li>Python REST API</li> <li>NodeJS Templating Engine</li> <li>MongoDB Service</li> </ol> <p>Developer systems can easily manage this workload for development.  And, it is useful to see a fully working application with data, frontend UIs, and backend services working together to solve a particular business problem.  </p> <p>At this level of integration, we'll also need to isolate dependencies on other services.  I usually create stub services that provide the full interface of the dependency while at the same time providing simulations of how the actual service will behave.</p>"},{"location":"blog/2024/01/08/development-integration/#isolate","title":"Isolate","text":"<p>At the first level, we should develop our service in complete isolation from all other services.  This gives us the freedom to prototype and to iterate on our service more rapidly.  We can use our standard tooling for development a service in a given framework.</p>"},{"location":"blog/2024/01/09/parallelized-docker-builds/","title":"Parallelized Docker Builds","text":""},{"location":"blog/2024/01/09/parallelized-docker-builds/#serial-builds","title":"Serial Builds","text":"<p>When creating a <code>Dockerfile</code>, we may be able to split up our work into multiple stages to distribute the work in parallel.  To simulate a large build, let's setup a <code>Dockerfile</code> with 3 <code>sleep 5</code> tasks in a single-stage build.</p> <pre><code>FROM debian:12.4\nWORKDIR /app\nRUN sleep 5 &amp;&amp; echo \"task1 Done\" &gt; /app/task1.log\nRUN sleep 5 &amp;&amp; echo \"task2 Done\" &gt; /app/task2.log\nRUN sleep 5 &amp;&amp; echo \"task3 Done\" &gt; /app/task3.log\n</code></pre> <p><pre><code>time DOCKER_BUILDKIT=1 docker build --tag serial_test .\n</code></pre> <pre><code>[+] Building 23.2s (12/12) FINISHED                                                                                       docker:default\n =&gt; [internal] load build definition from Dockerfile                                                                      0.0s\n =&gt; =&gt; transferring dockerfile: 288B                                                                                      0.0s\n =&gt; [internal] load .dockerignore                                                                                         0.0s\n =&gt; =&gt; transferring context: 2B                                                                                           0.0s\n =&gt; [internal] load metadata for docker.io/library/debian:12.4                                                            1.0s\n =&gt; [1/8] FROM docker.io/library/debian:12.4@sha256:bac353db4cc04bc672b14029964e686cd7bad56fe34b51f432c1a1304b9928da      4.1s\n =&gt; =&gt; resolve docker.io/library/debian:12.4@sha256:bac353db4cc04bc672b14029964e686cd7bad56fe34b51f432c1a1304b9928da      0.0s\n =&gt; =&gt; sha256:bac353db4cc04bc672b14029964e686cd7bad56fe34b51f432c1a1304b9928da 1.85kB / 1.85kB                            0.0s\n =&gt; =&gt; sha256:0dc902c61cb495db4630a6dc2fa14cd45bd9f8515f27fbb12e3d73a119d30bf1 529B / 529B                                0.0s\n =&gt; =&gt; sha256:2a033a8c63712da54b5a516f5d69d41606cfb5c4ce9aa1690ee55fc4f9babb92 1.46kB / 1.46kB                            0.0s\n =&gt; =&gt; sha256:bc0734b949dcdcabe5bfdf0c8b9f44491e0fce04cb10c9c6e76282b9f6abdf01 49.56MB / 49.56MB                          2.2s\n =&gt; =&gt; extracting sha256:bc0734b949dcdcabe5bfdf0c8b9f44491e0fce04cb10c9c6e76282b9f6abdf01                                 1.7s\n =&gt; [2/8] WORKDIR /app                                                                                                    0.2s\n =&gt; [3/8] RUN sleep 5 &amp;&amp; echo \"task1 Done\" &gt; /app/task1.log                                                               5.3s\n =&gt; [4/8] RUN sleep 5 &amp;&amp; echo \"task2 Done\" &gt; /app/task2.log                                                               5.4s\n =&gt; [5/8] RUN sleep 5 &amp;&amp; echo \"task3 Done\" &gt; /app/task3.log                                                               5.5s\n =&gt; [6/8] RUN cat /app/task1.log                                                                                          0.4s\n =&gt; [7/8] RUN cat /app/task2.log                                                                                          0.6s\n =&gt; [8/8] RUN cat /app/task3.log                                                                                          0.5s\n =&gt; exporting to image                                                                                                    0.2s\n =&gt; =&gt; exporting layers                                                                                                   0.1s\n =&gt; =&gt; writing image sha256:5a4ba49cbde2f6178268ca9d428fc6e0a831cdbaecb1a13a92730d473d21990a                              0.0s\n =&gt; =&gt; naming to docker.io/library/serial_test                                                                            0.0s\n\nWhat's Next?\n  1. Sign in to your Docker account \u2192 docker login\n  2. View a summary of image vulnerabilities and recommendations \u2192 docker scout quickview\n\nreal    0m23.594s\nuser    0m0.133s\nsys     0m0.141s\n</code></pre></p> <p>The serial build ran in 23.6 secs.</p>"},{"location":"blog/2024/01/09/parallelized-docker-builds/#parallel-builds","title":"Parallel Builds","text":"<p>Did you realize that Docker supports building stages in parallel?  This can greatly reduce build times depending on the service being built.  For example, let's take the following Dockerfile:</p> <pre><code>FROM debian:12.4 as task1\nWORKDIR /app\nRUN sleep 5 &amp;&amp; echo \"task1 Done\" &gt; /app/result.log\n\nFROM debian:12.4 as task2\nWORKDIR /app\nRUN sleep 5 &amp;&amp; echo \"task2 Done\" &gt; /app/result.log\n\nFROM debian:12.4 as task3\nWORKDIR /app\nRUN sleep 5 &amp;&amp; echo \"task3 Done\" &gt; /app/result.log\n\nFROM debian:12.4 as Final\n\nWORKDIR /app\nCOPY --from=task1 /app/result.log /app/task1.log\nCOPY --from=task2 /app/result.log /app/task2.log\nCOPY --from=task3 /app/result.log /app/task3.log\n</code></pre> <p><pre><code>time DOCKER_BUILDKIT=1 docker build --tag parallel_test .\n</code></pre> <pre><code>[+] Building 11.3s (12/12) FINISHED                                                                                           docker:default\n =&gt; [internal] load build definition from Dockerfile                                                                          0.0s\n =&gt; =&gt; transferring dockerfile: 510B                                                                                          0.0s\n =&gt; [internal] load .dockerignore                                                                                             0.0s\n =&gt; =&gt; transferring context: 2B                                                                                               0.0s\n =&gt; [internal] load metadata for docker.io/library/debian:12.4                                                                1.0s\n =&gt; [stage3 1/3] FROM docker.io/library/debian:12.4@sha256:bac353db4cc04bc672b14029964e686cd7bad56fe34b51f432c1a1304b9928da   4.0s\n =&gt; =&gt; resolve docker.io/library/debian:12.4@sha256:bac353db4cc04bc672b14029964e686cd7bad56fe34b51f432c1a1304b9928da          0.0s\n =&gt; =&gt; sha256:bac353db4cc04bc672b14029964e686cd7bad56fe34b51f432c1a1304b9928da 1.85kB / 1.85kB                                0.0s\n =&gt; =&gt; sha256:0dc902c61cb495db4630a6dc2fa14cd45bd9f8515f27fbb12e3d73a119d30bf1 529B / 529B                                    0.0s\n =&gt; =&gt; sha256:2a033a8c63712da54b5a516f5d69d41606cfb5c4ce9aa1690ee55fc4f9babb92 1.46kB / 1.46kB                                0.0s\n =&gt; =&gt; sha256:bc0734b949dcdcabe5bfdf0c8b9f44491e0fce04cb10c9c6e76282b9f6abdf01 49.56MB / 49.56MB                              2.1s\n =&gt; =&gt; extracting sha256:bc0734b949dcdcabe5bfdf0c8b9f44491e0fce04cb10c9c6e76282b9f6abdf01                                     1.7s\n =&gt; [stage3 2/3] WORKDIR /app                                                                                                 0.1s\n =&gt; [stage2 3/3] RUN sleep 5 &amp;&amp; echo \"stage2 Done\" &gt; /app/result.log                                                          5.4s\n =&gt; [stage1 3/3] RUN sleep 5 &amp;&amp; echo \"stage1 Done\" &gt; /app/result.log                                                          5.7s\n =&gt; [stage3 3/3] RUN sleep 5 &amp;&amp; echo \"stage3 Done\" &gt; /app/result.log                                                          5.7s\n =&gt; [final 3/5] COPY --from=stage1 /app/result.log /app/stage1.log                                                            0.0s\n =&gt; [final 4/5] COPY --from=stage2 /app/result.log /app/stage2.log                                                            0.0s\n =&gt; [final 5/5] COPY --from=stage3 /app/result.log /app/stage3.log                                                            0.0s\n =&gt; exporting to image                                                                                                        0.1s\n =&gt; =&gt; exporting layers                                                                                                       0.1s\n =&gt; =&gt; writing image sha256:9a00f8d862f8f76d2fdebcafa1128c4be35aea5088804c431dc44101907ccb8e                                  0.0s\n =&gt; =&gt; naming to docker.io/library/parallel_test                                                                              0.0s\n\nWhat's Next?\n  1. Sign in to your Docker account \u2192 docker login\n  2. View a summary of image vulnerabilities and recommendations \u2192 docker scout quickview\n\nreal    0m11.607s\nuser    0m0.060s\nsys     0m0.176s\n</code></pre></p> <p>The parallel build ran in 11.6 secs.</p> <p>Very cool!</p>"},{"location":"blog/2024/01/09/parallelized-docker-builds/#notes","title":"Notes","text":"<p>I was surprised to find this <code>Dockerfile</code> does not run the first and second stages if they're not used in the final stage:</p> <pre><code>FROM debian:12.4 as task1\nWORKDIR /app\nRUN sleep 5 &amp;&amp; echo \"task1 Done\" &gt; /app/result.log\n\nFROM debian:12.4 as task2\nWORKDIR /app\nRUN sleep 5 &amp;&amp; echo \"task2 Done\" &gt; /app/result.log\n\nFROM debian:12.4 as task3\nWORKDIR /app\nRUN sleep 5 &amp;&amp; echo \"task3 Done\" &gt; /app/result.log\n</code></pre> <p>Docker detects that the <code>task1</code> and <code>task2</code> stages are not used in the <code>task3</code> stage (through a <code>COPY --from=...</code>), so it skips them!</p> <p><pre><code>time DOCKER_BUILDKIT=1 docker build --tag skip_test .\n</code></pre> <pre><code>[+] Building 6.2s (7/7) FINISHED                                                                                             docker:default\n =&gt; [internal] load .dockerignore                                                                                            0.0s\n =&gt; =&gt; transferring context: 2B                                                                                              0.0s\n =&gt; [internal] load build definition from Dockerfile                                                                         0.0s\n =&gt; =&gt; transferring dockerfile: 310B                                                                                         0.0s\n =&gt; [internal] load metadata for docker.io/library/debian:12.4                                                               0.8s\n =&gt; [task3 1/3] FROM docker.io/library/debian:12.4@sha256:bac353db4cc04bc672b14029964e686cd7bad56fe34b51f432c1a1304b9928da   0.0s\n =&gt; CACHED [task3 2/3] WORKDIR /app                                                                                          0.0s\n =&gt; [task3 3/3] RUN sleep 5 &amp;&amp; echo \"task3 Done\" &gt; /app/result.log                                                           5.3s\n =&gt; exporting to image                                                                                                       0.0s\n =&gt; =&gt; exporting layers                                                                                                      0.0s\n =&gt; =&gt; writing image sha256:965ebe3258d6e1fb724b1130164dfdaae7ad2fef048f3fd21afcb8b9f9178f73                                 0.0s\n =&gt; =&gt; naming to docker.io/library/skip_test                                                                                 0.0s\n\nWhat's Next?\n  1. Sign in to your Docker account \u2192 docker login\n  2. View a summary of image vulnerabilities and recommendations \u2192 docker scout quickview\n\nreal    0m6.584s\nuser    0m0.066s\nsys     0m0.162s\n</code></pre></p> <p>Note the missing <code>task1</code> and <code>task2</code> stages in the output above.</p>"},{"location":"blog/2024/01/09/parallelized-docker-builds/#references","title":"References","text":"<ul> <li>Multi-stage | Docker Docs</li> </ul>"},{"location":"blog/2024/01/13/generate-a-cli-readme/","title":"Generate a CLI README","text":"<p>I find it useful sometimes to generate documentation directly from the CLI.</p> <p>Let's setup a simple <code>readme</code> <code>bash</code> function.  I'll use a simple command I use for working on this blog:  <code>run help</code>.  The command could be anything that you want to generate help from.</p> <pre><code>readme() {\n    cmd=\"run help\"\n    title=\"Run Help\"\n\n    html=help.html\n    pdf=help.pdf\n\n    # Install tools, if necessary.\n    if ! command -v aha &amp;&gt; /dev/null\n    then    \n        echo \"Installing aha\"\n        sudo apt install aha\n    fi\n    if ! command -v wkhtmltopdf &amp;&gt; /dev/null\n    then    \n        echo \"Installing wkhtmltopdf\"\n        sudo apt install wkhtmltopdf\n    fi\n\n    # Generate the html from the command.\n    script -qc \"$cmd\" /dev/null | aha --black --title \"$title\" &gt; \"$html\"\n\n    # Generate a pdf of the html file.\n    wkhtmltopdf -s Letter \"$html\" \"$pdf\"\n}\n</code></pre> <p></p> <p>For a little fun, I usually like throw in a little ASCII art.  For this one, I used:  Test ~330 fonts ASCII art generator | TextKool</p>"},{"location":"blog/2024/02/18/feature-flags/","title":"Feature Flags","text":"<p>One of the most foundational shifts I have implemented in the software development lifecycle is feature flags.  </p> <p>Traditionally, we would follow a process similar to:</p> <ol> <li>Design a feature.</li> <li>Develop a feature.</li> <li>Build the code for the feature.</li> <li>Deploy the feature to a QA environment for testing.</li> <li>Deploy the feature to production.</li> </ol> <p>But, that's a simplistic perspective of feature development.  Realistically, features follow a much more chaotic life cycle.  For example, a few iterations may look like:</p> <ol> <li>Design feature 1.</li> <li>Design feature 2 and develop feature 1.</li> <li>Design feature 3 and deploy feature 1 to QA.</li> <li>Develop feature 2 and feature 3, and fix feature 1.</li> <li>Fix feature 2 and deploy feature 1 and feature 3 to QA.</li> <li>Deploy feature 2 to QA.</li> <li>Deploy features 1, 2, and 3 to production.</li> </ol> <p>You can probably see how this can get unwieldy very quickly.  And, even in this complicated scenario, we haven't taken into account the need to change the final feature set at deployment.  Once a team experiences one deployment rollback, I'm sure they'll be looking to make code deployments more stable.</p>"},{"location":"blog/2024/02/18/feature-flags/#enter-flags","title":"Enter Flags","text":"<p>The concept of feature flags is really simple.  Just provide a simple data structure at runtime to enable or disable features.</p> <pre><code>{\n    \"ff_sample_feature\": true,\n    \"ff_another_feature\": true\n}\n</code></pre> <p>With this new data set, we can disable new features by default at deployment, and then at a later date, we can turn on the feature.  </p> <p>We have now separated <code>code</code> deployment from <code>feature</code> deployment.  </p>"},{"location":"blog/2024/02/18/feature-flags/#other-benefits","title":"Other benefits","text":"<p>Feature flags open the door to many other benefits.</p> <ol> <li>Technical Debt Management: You can wrap existing features scoped for deprecation in feature flags. You can leave them on by default and then schedule a time to turn them off.  At a later date, you can safely remove the code for the deprecated feature.</li> <li>Multi-Variate Flags: You can configure features with multi-variate flags.  For example, you could implement a color code feature flag to change the color of the navbar based on an environment.</li> <li>Feature Short Circuiting: With feature flags, the development team can take more risks by putting the riskier code behind feature flags.  For example, two code paths could provide the same business requirements, but the new code could have huge performance implications and pitfalls.  With a feature flag to toggle them, the team can schedule a time to turn switch the flag to gather some more data in production.</li> </ol> <p>And, the list goes on!</p>"},{"location":"blog/2024/04/21/python-function-protocols/","title":"Python Function Protocols","text":"<p>In TypeScript, it's common to pass a function directly as a parameter with the type information.</p> <pre><code>const addAdminUser = (\n    username: string,\n    addUserDB: (username: string, is_admin: boolean) =&gt; boolean): boolean =&gt; {\n    return addUserDB(username, true);\n}\n</code></pre> <p>The TypeScript implementation is rather straightforward.  You could imagine there is an expensive function with IO to the db that you would like to test.  By writing a function in this style, you can stub the <code>addUserDB</code> in your tests.  Then, you're testing library doesn't have to worry about maintaining connections to a live database.</p> <p>But, what would be the corollary for Python?  The most straightforward approach would be to use the <code>Callable</code> from the <code>typing</code> module:</p> <pre><code>from typing import Callable\n\n\ndef add_admin_user(\n    username: str, add_user_db: Callable[[str, bool], None]\n) -&gt; None:\n    add_user_db(username, True)\n</code></pre> <p>But, did you notice one critical difference with this annotation and the TypeScript version?  The arguments aren't named!  The <code>Callable</code> supports a list of argument types (<code>[str, bool]</code> in this case).  But, how what if you wanted to name the arguments?</p> <p>Enter <code>__call__</code>:</p> <pre><code>from typing import Protocol\n\n\nclass AddUserDb(Protocol):\n    def __call__(self, username: str, is_admin: bool) -&gt; None:\n        ...\n\n\ndef add_admin_user(\n    username: str, add_user_db: AddUserDb\n) -&gt; None:\n    add_user_db(username, True)\n</code></pre> <p>Obviously, the Python syntax is a little bit more verbose than the TypeScript version, but at least with this implementation you get the same benefits for test isolation.</p>"},{"location":"blog/2024/04/23/resilient-gateway/","title":"Resilient Gateway","text":"<p>Creating a \"Resilient Gateway\" using Nginx as a reverse proxy is a great way to ensure that your web infrastructure can handle failures gracefully. Let's explore how to configure Nginx to start up and remain operational even if the backend services are down or not yet up and running. This setup enhances the resilience and availability of your web applications, making them more robust against backend instabilities.</p>"},{"location":"blog/2024/04/23/resilient-gateway/#understand-the-challenge","title":"Understand: The Challenge","text":"<p>Typically, when Nginx is configured to proxy to a backend service and that service is unavailable at startup, Nginx will not come up!</p> <p>Let's take this <code>nginx.conf</code> as an example:</p> <pre><code>events {\n    worker_connections 1024;\n}\n\nhttp {\n    server {\n        listen 80;\n\n        location /flaky {\n            proxy_pass http://flaky:8000;\n        }\n    }\n}\n</code></pre> <p>If the <code>http://flaky:8000</code> service is not running, then Nginx will not start.  By default, Nginx requires all backend services to be up and running when it comes up.</p>"},{"location":"blog/2024/04/23/resilient-gateway/#the-solution-resilient-gateway","title":"The Solution: Resilient Gateway","text":"<p>To make our Nginx gateway resilient, we can use a combination of configuration strategies that include service discovery and DNS resolution. This setup ensures that Nginx can start up without needing the backend services to be available and can continue to operate if they go down.</p> <p>Step 1: Defining a Variable</p> <p>Nginx evaluates variables lazily. This means that the resolution of the hostname happens at the time of the request, allowing for dynamic changes in the backend's status.</p> <p><code>set $backend \"flaky:8000\";</code></p> <p>Here, <code>$backend</code> is a variable, and its resolution is deferred until a request is made. This setup allows for dynamic reconfiguration if the status of flaky changes.</p> <p>Step 2: Add the Internal DNS Resolver</p> <p><code>resolver 127.0.0.11 valid=30s;  # Docker's internal DNS resolver</code></p> <p>The <code>resolver</code> directive specifies the DNS server to use for name resolution since the service is discovered at the time of the request.</p> <p>All Together</p> <pre><code>events {\n    worker_connections 1024;\n}\n\nhttp {\n    server {\n        listen 80;\n\n        set $backend \"flaky:8000\";\n\n        location /flaky {\n            proxy_pass http://$backend;\n        }\n    }\n}\n</code></pre> <p>In this configuration, if the flaky service is down, users will encounter an error when trying to access it through the gateway.  Serving error pages is by far a better experience than not responding at all.  Also, other services may be up and running and can respond to requests even if a particular backend service is down.</p>"},{"location":"blog/2024/04/23/resilient-gateway/#conclusion","title":"Conclusion","text":"<p>By making these changes, your Nginx reverse proxy becomes a \"Resilient Gateway\" that enhances the availability and stability of your applications. This configuration is especially useful in microservices architectures where individual service availability can vary.</p> <p>This approach not only minimizes downtime but also improves the overall user experience by providing a more reliable entry point to your backend services. Whether you're managing a complex system or just want to ensure uptime for critical applications, these strategies can help your infrastructure become more resilient against failures.</p>"},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/archive/2021/","title":"2021","text":""},{"location":"blog/archive/2020/","title":"2020","text":""},{"location":"blog/archive/2019/","title":"2019","text":""},{"location":"blog/category/nginx/","title":"Nginx","text":""},{"location":"blog/category/microservices/","title":"Microservices","text":""},{"location":"blog/category/python/","title":"Python","text":""},{"location":"blog/category/devops/","title":"DevOps","text":""},{"location":"blog/category/cli/","title":"CLI","text":""},{"location":"blog/category/docker/","title":"Docker","text":""},{"location":"blog/category/bash/","title":"Bash","text":""},{"location":"blog/category/git/","title":"Git","text":""},{"location":"blog/category/frontend/","title":"Frontend","text":""},{"location":"blog/category/fsharp/","title":"FSharp","text":""},{"location":"blog/category/architecture/","title":"Architecture","text":""},{"location":"blog/category/sql-server/","title":"SQL SERVER","text":""},{"location":"blog/category/azure/","title":"Azure","text":""},{"location":"blog/category/typescript/","title":"TypeScript","text":""},{"location":"blog/category/haskell/","title":"Haskell","text":""},{"location":"blog/category/t-sql/","title":"T-SQL","text":""},{"location":"blog/category/javascript/","title":"JavaScript","text":""},{"location":"blog/category/testing/","title":"Testing","text":""},{"location":"blog/page/2/","title":"Blog","text":""},{"location":"blog/page/3/","title":"Blog","text":""},{"location":"blog/page/4/","title":"Blog","text":""}]}