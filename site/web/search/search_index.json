{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Joel Vandiver's Blog","text":"<p>I intend to use this blog for learning.  I do not seek to teach for teaching's sake, but rather I teach to learn.  I have so much to unpack, so much to learn, and so much to get excited about!  And, I can't wait to dive in.   </p>"},{"location":"about/","title":"About","text":"<p>Hi, I'm Joel. I started coding in 2008 with JavaScript as a high school Math teacher.  I had been building a website (back in the Internet dark ages) for my students to learn from.  By this point, I had already flipped my classroom where students would learn from my online videos (see my original youtube account) and then they completed \"homework\" in the classroom with me to help as their \"tutor\".  </p> <p>The deeper I went with coding, the more successful I became at teaching.  I had gotten to the point where I started learning C++!  I figured, \"Eh, anyone who's serious about coding has to learn C++.\"  </p> <p>Then, I got the opportunity to work professionally in software engineering.  I started my career in C# and SQL SERVER.  Day after day, I would slog through raw SQL queries as I learned to express my thoughts with code.  Then, I quickly sought to automate some of my work through an ASP.NET MVC app written in C#. After a few years, I brought JavaScript back to my daily routine with AngularJS and jQuery.  </p> <p>Then, I had the opportunity to move into a position at a bigger company with a larger space to grow in.  I continued developing in C#, SQL SERVER, and JavaScript, but I quickly transitioned to using F# full-time.  </p> <p>I really love F#.  There's something about it's syntax that feels more natural as a mathematician.  I spent several years just hanging out with F# and SQL SERVER.</p> <p>But, then, in order to grow, I needed to move to a larger team with an even greater impact within the company.  Through this team, I've grafted in Python, TypeScript, ReactJS, NodeJS, PostgreSQL, and Docker (to name a few) into my development experience.  </p> <p>Now, I really love Python.  I was surprised to learn that Python supports type annotations and comprehension expressions.  Both of these features are critical for functional programming.</p> <p>My career has been a wild ride of constant learning, and I've got so much more to explore!</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2019/09/04/nasa-apod/","title":"NASA APOD","text":"<p>I wanted to add something interesting to my blog, and then I remembered that NASA has an astronomy picture of the day (APOD).</p> <p>https://api.nasa.gov/</p> <p>After requesting my own api key, I was able to issue GET requests to get the photo of the day: <code>fetch(\"https://api.nasa.gov/planetary/apod?api_key={API_KEY}\")</code>.</p> <p>Here's a sample from today's response:</p> <p>Json</p> <pre><code>{\n   \"date\": \"2019-09-04\",\n   \"explanation\": \"Will the spider ever catch the fly? Not if both are large emission nebulas toward the constellation of the Charioteer (Auriga).  The spider-shaped gas cloud on the left is actually an emission nebula labelled IC 417, while the smaller fly-shaped cloud on the right is dubbed  NGC 1931 and is both an emission nebula and a reflection nebula.  About 10,000 light-years distant, both nebulas harbor young, open star clusters. For scale, the more compact NGC 1931 (Fly) is about 10 light-years across. The featured picture in scientifically-assigned, infrared colors combines images from the Spitzer Space Telescope and the Two Micron All Sky Survey (2MASS).  Spitzer is celebrating its 16th year orbiting the Sun near the Earth.    APOD in other languages: Arabic, Catalan, Chinese (Beijing), Chinese (Taiwan), Croatian, Czech, Dutch, Farsi, French, French, German, Hebrew, Indonesian, Japanese, Korean, Montenegrin, Polish, Russian, Serbian, Slovenian,  Spanish and Ukrainian\",\n   \"hdurl\": \"https://apod.nasa.gov/apod/image/1909/SpiderFly_Spitzer2Mass_4165.jpg\",\n   \"media_type\": \"image\",\n   \"service_version\": \"v1\",\n   \"title\": \"The Spider Nebula in Infrared\",\n   \"url\": \"https://apod.nasa.gov/apod/image/1909/SpiderFly_Spitzer2Mass_960.jpg\"\n}\n</code></pre> <p>With that response, I simply need to extract the url and create an <code>img</code> element when the <code>media_type === \"image\"</code>.</p> <p>Here's the full example:</p> <p>Html</p> <pre><code>&lt;div id=\"nasa-img-container\"&gt;&lt;/div&gt;\n</code></pre> <p>JavaScript</p> <pre><code>(function () {\n    fetch(\"https://api.nasa.gov/planetary/apod?api_key={API_KEY}\")\n        .then(function (response) {\n            return response.json();\n        })\n        .then(function (json) {\n            if (!json.media_type === \"image\") { return; }\n            var src = json.url;\n            var container = document.getElementById(\"nasa-img-container\");\n            var img = document.createElement('img');\n            img.src = src;\n            container.title = json.explanation;\n            container.appendChild(img);\n        });\n})();\n</code></pre> <p>Here's my favorite photo I've found in APOD so far:</p> <p></p>"},{"location":"blog/2019/12/07/docker-sql-server/","title":"SQL SERVER on Docker","text":"<p>Setting up SQL SERVER on a Docker Container is so easy that I cringe at all of the times that I have installed a full-blown SQL SERVER instance on my systems in the past.  </p> <p>What used to take a few hours with several touch points, now just takes a couple of commands and a few mins.</p> <p>Also, since this is on Docker, SQL SERVER is fully isolated from the rest of my system.</p>"},{"location":"blog/2019/12/07/docker-sql-server/#lets-get-this-done","title":"Let's Get This Done","text":"<p>Install the latest with <code>docker pull</code>:</p> <pre><code>docker pull mcr.microsoft.com/mssql/server:2017-latest\n</code></pre> <p>Set the SA password:</p> <pre><code>docker run -e \"ACCEPT_EULA=Y\" -e \"SA_PASSWORD=&lt;YourStrong@Passw0rd&gt;\" `\n   -p 1433:1433 --name sql1 `\n   -d mcr.microsoft.com/mssql/server:2017-latest\n</code></pre> <p>Now, you are free to connect to it with sqlcmd, or you can connect directly from SQL SERVER Management Studio:</p> <p>Be sure to use the <code>hostname</code> as the Server name.</p> <p>This entire setup took 5 mins and is amazingly simple!</p>"},{"location":"blog/2019/12/07/docker-sql-server/#refs","title":"Refs","text":"<ul> <li>https://docs.microsoft.com/en-us/sql/linux/quickstart-install-connect-docker?view=sql-server-2017&amp;pivots=cs1-powershell</li> </ul>"},{"location":"blog/2019/12/07/meta-programming---code-to-write-code/","title":"META Programming - Code to Write Code","text":"<p>As I'm sure any .NET programmer would know, .NET offers a pretty expansive reflection api.  Asking a system to provide reflection information is both a powerful and expensive technique.  It can dramatically reduce the quantity of code, but it comes with the extra overhead in runtime performance.  </p> <p>I have come across situations many times where the code I am writing is very repetitive, and the only way to stop the repetition at runtime is with reflection.  </p>"},{"location":"blog/2019/12/07/meta-programming---code-to-write-code/#validation-example","title":"Validation Example","text":"<p>One common example is validation against table schemas.  If you need to ensure the incoming .NET object will not raise a SQL exception, then you will typically have to check each of the properties on the record per table.  </p> <p>Take the following <code>dbo.visits</code> table:</p> <pre><code>CREATE TABLE dbo.visits (\n    visit_id INT PRIMARY KEY IDENTITY (1, 1),\n    first_name VARCHAR (50) NOT NULL,\n    last_name VARCHAR (50) NOT NULL,\n    visited_at DATETIME,\n    phone VARCHAR(20)\n);\n</code></pre> <p>and corresponding F# record:</p> <pre><code>type Visits =\n    {   visit_id: int\n        first_name: string\n        last_name: string\n        visite_at: DateTime\n        phone: string\n        store_id: int\n        }\n</code></pre> <p>To ensure a successful transaction with this record against the database, then you will need to check:</p> <ol> <li><code>first_name</code> is not null and is 50 characters or less.</li> <li><code>last_name</code> is not null and is 50 characters or less.</li> <li><code>visited_at</code> is greater than the SQL minimum date.</li> <li>...</li> </ol> <p>This might look something like:</p> <pre><code>let validate visit = \n    let errors = \n        [\n            \"first_name\", visit.first_name &lt;&gt; null &amp;&amp; visit.first_name.Length &lt;= 50, \"The first_name is required and must be less than or equal to 50 characters.\"\n            \"last_name\", visit.last_name &lt;&gt; null &amp;&amp; visit.last_name.Length &lt;= 50, \"The last_name is required and must be less than or equal to 50 characters.\"\n            // ...\n        ]\n\n    match errors with \n    | [] -&gt; Ok visit\n    | _ -&gt; Error errors\n</code></pre> <p>Note:  You will need to include unit tests of the validation rules from the table schema.</p> <p>You can see from the <code>validate</code> function above that the pattern of checking each of the fields is very repetitive.</p> <p>'Ain't no body got time for that!'</p>"},{"location":"blog/2019/12/07/meta-programming---code-to-write-code/#keep-it-dry","title":"Keep it D.R.Y.","text":""},{"location":"blog/2019/12/07/meta-programming---code-to-write-code/#information_schema","title":"INFORMATION_SCHEMA","text":"<p>The information that we are extracting from the <code>CREATE TABLE</code> can be found in the <code>INFORMATION_SCHEMA</code> in SQL SERVER.  (Other database providers have their own schema querying views.)</p> <pre><code>SELECT COLUMN_NAME, IS_NULLABLE, DATA_TYPE, CHARACTER_MAXIMUM_LENGTH\nFROM INFORMATION_SCHEMA.COLUMNS\n</code></pre> COLUMN_NAME IS_NULLABLE DATA_TYPE CHARACTER_MAXIMUM_LENGTH visit_id NO int NULL first_name NO varchar 50 last_name NO varchar 50 visited_at YES datetime NULL phone YES varchar 20"},{"location":"blog/2019/12/07/meta-programming---code-to-write-code/#f-schema","title":"F# Schema","text":"<p>Use your standard ORM to query this data out in an <code>fsx</code> script into an F# type:</p> <pre><code>type ColumnSchema =\n    {   COLUMN_NAME: string\n        IS_NULLABLE: string\n        DATA_TYPE: string\n        CHARACTER_MAXIMUM_LENGTH: int option\n        }\n\nlet meta() = \n\n    // TODO: Run query...\n\n    [\n        {   COLUMN_NAME = \"visit_id\"\n            IS_NULLABLE = \"NO\"\n            DATA_TYPE = \"int\"\n            CHARACTER_MAXIMUM_LENGTH = None\n            }\n        {   COLUMN_NAME = \"first_name\"\n            IS_NULLABLE = \"NO\"\n            DATA_TYPE = \"varchar\"\n            CHARACTER_MAXIMUM_LENGTH = Some 50\n            }\n        {   COLUMN_NAME = \"last_name\"\n            IS_NULLABLE = \"NO\"\n            DATA_TYPE = \"varchar\"\n            CHARACTER_MAXIMUM_LENGTH = Some 50\n            }\n        {   COLUMN_NAME = \"visited_at\"\n            IS_NULLABLE = \"YES\"\n            DATA_TYPE = \"datetime\"\n            CHARACTER_MAXIMUM_LENGTH = None\n            }\n        {   COLUMN_NAME = \"phone\"\n            IS_NULLABLE = \"YES\"\n            DATA_TYPE = \"varchar\"\n            CHARACTER_MAXIMUM_LENGTH = Some 20\n            }\n    ]\n</code></pre>"},{"location":"blog/2019/12/07/meta-programming---code-to-write-code/#lets-get-meta","title":"Let's Get META","text":"<p>Convert the repetitive code above into a template.  This is where we treat code as data!</p> <pre><code>let requiredStringTemplate (x: ColumnSchema list = \n    let name = x.COLUMN_NAME\n    let max = x.CHARACTER_MAXIMUM_LENGTH.Value\n    sprintf \"\"\"\n    \"%s\", x.%s &lt;&gt; null &amp;&amp; x.%s.Length &lt;= %i, \"The %s is required and must be less than or equal to %i characters.\"\n    \"\"\" name name name max name max\n\n// TODO:  Provide other rules.\n\nlet template (xs: ColumnSchema list) = \n    let rules = \n        xs \n        |&gt; List.map(fun x -&gt; \n            match (x.DATA_TYPE, x.IS_NULLABLE, x.CHARACTER_MAXIMUM_LENGTH) with \n            | (\"varchar\", \"NO\", Some _) -&gt; requiredStringTemplate x |&gt; Some\n            // TODO:  Call other rules.\n            | _ -&gt; None)\n        |&gt; List.filter Option.isSome\n        |&gt; List.map Option.get\n        |&gt; List.fold (fun a b -&gt; a + \"\\r\\n            \" + b) \"\"\n    \"\"\"\nlet validate x = \n    let errors = \n        [\n            %s\n        ]\n\n    match errors with \n    | [] -&gt; Ok x\n    | _ -&gt; Error errors\n\"\"\" rules\n</code></pre>"},{"location":"blog/2019/12/07/meta-programming---code-to-write-code/#meta-benefits","title":"META Benefits","text":"<ol> <li>META Test - We can now safely delete our unit tests since we would only need to test the META code.</li> <li>Schema Changes - As the schema evolves in the future, you can simply run this job to generate the final validation code.  This ensures that the database schema is the lone, authoritative source of schema information.</li> </ol>"},{"location":"blog/2019/12/07/meta-programming---code-to-write-code/#side-note","title":"Side Note","text":"<p>If you prefer to keep your database schema dumb, then you can move the authoritative source to another format such as <code>yaml</code>.  Then, use that source as your input into the <code>ColumnSchema</code> above.</p> <p>Pretty sweet!</p>"},{"location":"blog/2020/02/24/t-sql-recursive-cte-common-table-expression/","title":"T-SQL Recursive CTE (*Common Table Expression*)","text":"<p>CTEs are useful for modeling queries that operate on recursive structures.</p> <p>I often have the need to categorize or group an entity type with a tree structure.  This requirement can be met with a simple table and CTE to order the tree structure.</p> <p>Let's take the following for example:</p> <pre><code>/*\n    Create a simple `Category` table for storing a \n    tree structure of categories in a flat table.\n\n    Use the `ParentId` to find the parent of the \n    category.\n*/\nCREATE TABLE Category (\n    Id uniqueidentifier NOT NULL\n    , ParentId uniqueidentifier NULL\n    , Title nvarchar(75) NOT NULL\n);\nGO\n\n/*\n    Use a CTE to order the categories\n    by level and parent.\n*/\n;WITH\u00a0Tree(Id,\u00a0ParentId,\u00a0Title,\u00a0[Level])\nAS\u00a0(\n\u00a0\u00a0\u00a0\u00a0SELECT\u00a0Id\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0,\u00a0ParentId\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0,\u00a0Title\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0,\u00a00\u00a0AS\u00a0[Level]\n\u00a0\u00a0\u00a0\u00a0FROM\u00a0Category\u00a0p\n    WHERE ParentId IS NULL\n\u00a0\u00a0\u00a0\u00a0UNION\u00a0ALL\u00a0\n\u00a0\u00a0\u00a0\u00a0SELECT\u00a0t.Id\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0,\u00a0t.ParentId\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0,\u00a0t.Title\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0,\u00a0tree.Level\u00a0+\u00a01\u00a0[Level]\n\u00a0\u00a0\u00a0\u00a0FROM\u00a0Category\u00a0t\n\u00a0\u00a0\u00a0\u00a0JOIN\u00a0Tree\u00a0tree\u00a0ON\u00a0tree.Id\u00a0=\u00a0t.ParentId\n)\nSELECT\u00a0*\u00a0\nFROM\u00a0Tree\nORDER\u00a0BY\u00a0[Level],\u00a0ParentId\n</code></pre>"},{"location":"blog/2020/04/26/haskell-setup/","title":"Haskell Setup","text":"<p>In my journey down the path of purity in functional programming, I have inevitably merged with the path of <code>Haskell</code> programming.  <code>Haskell</code> seems like an amazing language.  I started my career rooted in Mathematics, and I feel at home coding in the <code>Haskell</code> programming language.  </p> <p>But, I've also seen the pain of setting up my dev system for multiple types of coding environments.  For quick console coding, I'd really like to get off the train of managing installs on my system.  </p>"},{"location":"blog/2020/04/26/haskell-setup/#docker","title":"Docker","text":"<p>Let's setup a <code>Haskell</code> dev environment with Docker to isolate the environment.</p> <p>Requirements</p> <ol> <li>The <code>Haskell</code> coding environment should be isolated from the dev system.</li> <li>The *.hs code files should be reloaded without having to rebuild Docker images.</li> </ol> <pre><code># ${PWD} is the current directory in PowerShell\ndocker run -it --rm -v ${PWD}:/app haskell:8\n\n:load app/main.hs\n</code></pre> <p>Let's get to <code>Haskell</code> code!</p>"},{"location":"blog/2020/05/12/use-powershell-to-spin-up-an-integration-test-api/","title":"Use PowerShell to Spin Up an Integration Test API","text":"<p>In developing CI/CD pipelines, I have often come across the need to spin up an api just to run integration tests against.  Below is a PowerShell script I came up with to startup an api on another process, check that it is alive, and then run the integration tests against that api.  </p> <p>Works like a charm!</p> <pre><code># Assume the API is not alive.\n$isAlive\u00a0=\u00a0$false\n\nSet-Location \"path/to/integration/test/api\"\n\n#\u00a0Start Test API\u00a0in the Background\nstart\u00a0dotnet\u00a0run\n\n#\u00a0Wait\u00a0for\u00a0Process\n\n# Define a Check-Process Function\n[Net.ServicePointManager]::SecurityProtocol\u00a0=\u00a0[Net.SecurityProtocolType]::Tls12\nFunction\u00a0Check-Process()\u00a0{\n\u00a0\u00a0\u00a0\u00a0return\u00a0Invoke-RestMethod\u00a0-Method\u00a0get\u00a0`\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-ContentType\u00a0'Application/Json'\u00a0`\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-Uri\u00a0\"https://localhost:5001/health\"\u00a0`\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-ErrorAction\u00a0SilentlyContinue\n}\n\n# Check if the API is alive at most 10 times.\nFor\u00a0($i\u00a0=\u00a00;\u00a0$i\u00a0-le\u00a010;\u00a0$i++)\u00a0{\n\u00a0\u00a0\u00a0\u00a0Write-Output\u00a0\"Sleeping\u00a0for\u00a05\u00a0seconds.\"\n\u00a0\u00a0\u00a0\u00a0Start-Sleep\u00a0-Seconds\u00a05\n\u00a0\u00a0\u00a0\u00a0Write-Output\u00a0\"Trying\u00a0https://localhost:5001/health\"\n\u00a0\u00a0\u00a0\u00a0$isAlive\u00a0=\u00a0Check-Process\u00a0-eq\u00a0\"Healthy\"\n\u00a0\u00a0\u00a0\u00a0if\u00a0($isAlive)\u00a0{\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Break\n\u00a0\u00a0\u00a0\u00a0}\n}\n\n# Stop if the api is not alive at this point.\nif\u00a0(-Not($isAlive))\u00a0{\n\u00a0\u00a0\u00a0\u00a0Throw\u00a0\"The\u00a0Integration\u00a0Src\u00a0API\u00a0is\u00a0not\u00a0alive\u00a0after\u00a050\u00a0seconds.\"\n}\n\nSet-Location \"path/to/integration/tests\"\n\n#\u00a0Run\u00a0Integration Tests\ndotnet\u00a0run\n\n#\u00a0Get\u00a0Process\u00a0Id so that we can kill it later.\n$api_pid\u00a0=\u00a0(Get-NetTCPConnection\u00a0-LocalPort\u00a05001).OwningProcess[0]\n\n#\u00a0Kill\u00a0the API\u00a0Process\nStop-Process\u00a0-Id\u00a0$api_pid\n</code></pre>"},{"location":"blog/2020/07/07/apod-azure-function/","title":"APOD Azure Function","text":"<p>I've written about setting up a JS photo script with NASA Astronomy Photo of the Day (APOD).  But, to be honest, I've gotten a little greedy with these photos.  I like that I can go to my blog and get the photo of the day, but I would prefer to keep a cache of previous photos of the day!  </p>"},{"location":"blog/2020/07/07/apod-azure-function/#enter-azure-functions","title":"Enter Azure Functions","text":"<p>I already store my blog on an Azure storage account.  Wouldn't it be great if I could automatically get the APOD photo and put it into that storage account?</p> <p>Since I wrote the original script in JavaScript, I opted for TypeScript as the Azure Function language of choice.  </p> <p>After pulling down an auto-generated Azure Function into VS Code, I did the following.</p> <p>First, I setup the dependencies:</p> <p>package.json:</p> <pre><code>  \"dependencies\": {\n    \"@azure/storage-blob\": \"^12.2.0-preview.1\",\n    \"node-fetch\": \"^2.6.0\"\n  },\n  \"devDependencies\": {\n    \"@azure/functions\": \"^1.0.2-beta2\",\n    \"@types/node-fetch\": \"^2.5.7\",\n    \"typescript\": \"^3.3.3\"\n  }\n</code></pre> <p>Second, I defined the script.  </p> <pre><code>import { AzureFunction, Context } from \"@azure/functions\"\nimport { ContainerClient, StorageSharedKeyCredential } from \"@azure/storage-blob\";\nimport fetch from \"node-fetch\";\n\nconst getFileName = function (url: string) {\n    const s = url.split(\"/\");\n    const l = s.length;\n    return s[l - 1];\n};\n\nconst saveImg = async function (url: string, context: Context) {\n    var timeStamp = new Date().toISOString();\n    context.log('Saving url:  ' + url, timeStamp);\n    const creds = new StorageSharedKeyCredential(\"{STORAGE_ACCOUNT_NAME}\", \"{STORAGE_ACCOUNT_KEY}\");\n    const client = new ContainerClient(\"{STORAGE_ACCOUNT_URI}/assets/img/apod\", creds)\n    const filename = getFileName(url);\n    const blob = client.getBlobClient(filename);\n    await blob.syncCopyFromURL(url);\n};\n\nconst sync = async function (context: Context) {\n    const json_url = \"https://api.nasa.gov/planetary/apod?api_key={APOD_API_KEY}\";\n    context.log(\"Fetching \" + json_url);\n    await fetch(json_url)\n        .then(function (response) { return response.json(); })\n        .then(function (json) {\n            const timeStamp = new Date().toISOString();\n            const img_url = json.url;\n            if (json.media_type !== \"image\") {\n                context.log('The json response is not an image: ' + img_url, timeStamp);\n                return;\n            }\n            context.log(\"Fetched \" + json_url);\n            return saveImg(img_url, context);\n        });\n};\n\nconst timerTrigger: AzureFunction = async function (context: Context, myTimer: any): Promise&lt;void&gt; {\n    const timeStamp = new Date().toISOString();\n    context.log('Syncing', timeStamp);\n    await sync(context);\n};\n\nexport default timerTrigger;\n</code></pre> <p>Third, I set the function to run on a timer since this is the \"Photo of the Day\".  </p> <p>That's it!  Now, I have a simple Azure Function running in the cloud, storing the photos in the cloud, of the astronomy beyond the clouds!  Nice!</p>"},{"location":"blog/2020/08/02/sql-server-on-docker-with-sqlcmd/","title":"SQL SERVER on Docker with SQLCMD","text":"<p>Let's revisit a previous post about using Docker to create a SQL SERVER image.  Previously, I connected to the DB with SQL SERVER Management Studio (SSMS), but now, I'd like to use <code>sqlcmd</code> to avoid running other tools.</p> <p>Let's get the latest mssql image for Linux this time from https://hub.docker.com/_/microsoft-mssql-server.</p> <pre><code># Pull the Image\ndocker pull mcr.microsoft.com/mssql/server:2019-CU5-ubuntu-16.04\n\n# Run the Container\ndocker run -e 'ACCEPT_EULA=Y' -e 'SA_PASSWORD=...' -p 1433:1433 --name sql1 -d mcr.microsoft.com/mssql/server:2019-CU5-ubuntu-16.04\n\n# Start an Interactive Shell\ndocker exec -it sql1 /bin/bash\n\n# Test SQLCMD\nopt/mssql-tools/bin/sqlcmd -?\n</code></pre> <p>Output</p> <pre><code>Microsoft (R) SQL Server Command Line Tool\nVersion 17.5.0002.1 Linux\nCopyright (C) 2017 Microsoft Corporation. All rights reserved.\n\nusage: sqlcmd            [-U login id]          [-P password]\n  [-S server or Dsn if -D is provided]\n  [-H hostname]          [-E trusted connection]\n  [-N Encrypt Connection][-C Trust Server Certificate]\n  [-d use database name] [-l login timeout]     [-t query timeout]\n  [-h headers]           [-s colseparator]      [-w screen width]\n  [-a packetsize]        [-e echo input]        [-I Enable Quoted Identifiers]\n  [-c cmdend]\n  [-q \"cmdline query\"]   [-Q \"cmdline query\" and exit]\n  [-m errorlevel]        [-V severitylevel]     [-W remove trailing spaces]\n  [-u unicode output]    [-r[0|1] msgs to stderr]\n  [-i inputfile]         [-o outputfile]\n  [-k[1|2] remove[replace] control characters]\n  [-y variable length type display width]\n  [-Y fixed length type display width]\n  [-p[1] print statistics[colon format]]\n  [-R use client regional setting]\n  [-K application intent]\n  [-M multisubnet failover]\n  [-b On error batch abort]\n  [-D Dsn flag, indicate -S is Dsn]\n  [-X[1] disable commands, startup script, environment variables [and exit]]\n  [-x disable variable substitution]\n  [-g enable column encryption]\n  [-G use Azure Active Directory for authentication]\n  [-? show syntax summary]\n</code></pre> <pre><code># Run SQLCMD\nopt/mssql-tools/bin/sqlcmd -S localhost -U sa -P Password12345 -Q \"SELECT * FROM INFORMATION_SCHEMA.TABLES\"\n</code></pre> TABLE_CATALOG TABLE_SCHEMA TABLE_NAME TABLE_TYPE master dbo spt_fallback_db BASE TABLE master dbo spt_fallback_dev BASE TABLE master dbo spt_fallback_usg BASE TABLE master dbo spt_values VIEW master dbo spt_monitor BASE TABLE master dbo MSreplication_options BASE TABLE <p>Very Nice!</p>"},{"location":"blog/2020/09/12/avoid-mockingstubbing-services/","title":"Avoid Mocking/Stubbing Services","text":"<p>Let's say that we have a function that calls an external service:</p> <pre><code>let sayHelloToCurrentUser () = \n    // Call External Service\n    AccountService.getUsername()\n    |&gt; sprintf \"Hello %s!\"\n\n// Integration Test\nlet [&lt;Fact&gt;] ``should say Hello to the name``() =\n    sayHelloToCurrentUser()\n    |&gt; equal \"Hello Joel!\"\n</code></pre> <p>That may look simple enough, except that this function is technically an integration function since it makes an external call.  It is difficult to guarantee the service availability and connections in a CI (continuous integration) environment.  </p> <p>It is best to isolate the external calls in UNIT tests so that the run time only has to have the code itself to run the tests.</p> <p>There are two primary options you can use to isolate the external call.</p> <ol> <li>Put the service call behind an interface and provide fake implementations with mocking or stubbing.</li> <li>Isolate the \"testable\" functions from the service connections.</li> </ol> <p>For the first approach, we can provide a callback function to <code>getUsername</code>.</p> <pre><code>let sayHelloToCurrentUser (getUsername: unit -&gt; string) = \n    getUsername()\n    |&gt; sprintf \"Hello %s!\"\n\n// UNIT Test\nlet [&lt;Fact&gt;] ``should say Hello to the name``() =\n    // Function Stub\n    fun _ -&gt; \"Joel\"\n    |&gt; sayHelloToCurrentUser\n    |&gt; equal \"Hello Joel!\"\n</code></pre> <p>The complexity of stubbing a service can grow rather quickly, though.  You can probably imagine that the AccountService has lots of methods exposed.  We would end up putting the service behind a .NET interface so that we can provide a fake implementation in our UNIT tests.  </p> <p>But, there's a simpler way.  </p> <pre><code>// Lib\nlet sayHello name = sprintf \"Hello %s!\" name\n\n// Composition\nlet sayHelloToCurrentUser = AccountService.getUsername &gt;&gt; sayHello\n\n// UNIT Test\nlet [&lt;Fact&gt;] ``should say Hello to the name``() =\n    \"Joel\"\n    |&gt; sayHello\n    |&gt; equal \"Hello Joel!\"\n</code></pre> <p>Now, we are simply isolating the integration point to a module devoted to composition.  </p> <p>Now, we just have a simple function in our <code>Lib</code>.  It doesn't require any stubbing or mocking.  Very nice indeed!</p>"},{"location":"blog/2021/06/08/git-flow/","title":"Git Flow","text":"<p>First off, you're doing great! You are doing your best to keep track of the changes to your system.  The people on your team know what's going on ... usually.  Code conflicts only turn into actual conflicts sporadically.  But, hey, that's just part of the game, isn't it?  You only run <code>git blame</code> sometimes - no, not to shame anyone, just blame.  </p> <p>But, we can do better!</p> <p>Git is a powerful (if not all-consuming) source control system.  I've been using it for a number of years now, and my particular workflow with Git has changed pretty dramatically over time.  </p>"},{"location":"blog/2021/06/08/git-flow/#git-push-pull-flow","title":"Git Push-Pull Flow","text":"<p>In my early days of learning and using Git, I opted for the simple push-pull workflow.  The workflow was simply:</p> <ol> <li>Fetch other developers' code.</li> <li>Merge the changes.</li> <li>Resolve any conflicts.</li> <li>Commit the changes.</li> <li>Push the new commit.</li> </ol> <p>If there are no conflicts, then the above workflow shortens down to:</p> <ol> <li>Fetch other developers' code.</li> <li>Merge the changes with fast-forward. (No new commit!)</li> </ol> <p>This works fine for small systems but begins to have difficulties with larger systems.  Systems grow in complexity in a number of ways, including:  </p> <ul> <li>number of developers</li> <li>number of existing features</li> <li>number of features in development</li> <li>number of distributed sub-systems</li> <li>number of languages and frameworks</li> <li>number of supported environments </li> <li>different users per environment </li> <li>quantity of code</li> <li>etc.</li> </ul>"},{"location":"blog/2021/06/08/git-flow/#noisy-commits","title":"Noisy Commits","text":"<p>The difficulty comes from providing a meaningful commit log of the changes over time in complex systems.</p>"},{"location":"blog/2021/06/08/git-flow/#feature-branch-commits","title":"Feature Branch Commits","text":"<p>Developers should be branching their code while working through a feature.  And, they should be trying things out in that branch.  This can and should lead to learning moments.  Learning means the developer found a better way.  Commit after commit may say something like \"Changed this thing\" and \"Undid my last change because of this reason\".  </p> <p>Anger-Danger:  Coming across the previous commit may lead to unexpected conclusions about the current state of the system. What do you mean you \"Changed this thing\"?  </p>"},{"location":"blog/2021/06/08/git-flow/#merge-commits","title":"Merge Commits","text":"<p>Probably a little less egregious but nonetheless annoying is the \"Merge Commit\" commit (ironic redundancy?).  It's likely that most developers would agree that a commit titled, \"Merge Commit\" is generally not helpful.  So what?  You merged the branches.  What did you actually do to the system?</p>"},{"location":"blog/2021/06/08/git-flow/#cleanup-commits","title":"\"CLEANUP\" Commits","text":"<p>Should developers be cleaning their code?  Of course they should!  How helpful is a commit that says something like, \"Removed unnecessary whitespace; provided better variable names; did such and such and yada yada.\"</p> <p>Ha!  I even went a step further and waited for a valid commit message to commit cleanup changes.  Garbage!</p>"},{"location":"blog/2021/06/08/git-flow/#squash-rebase-flow","title":"Squash-Rebase Flow","text":"<p>So, how do we get rid of those pesky commits?  Well, squash them or rebase your work on top of another developer's work.  Granted both ideas may seem a little less intuitive at first from Push-Pull, but hang in there.  </p>"},{"location":"blog/2021/06/08/git-flow/#squash-commit","title":"Squash Commit","text":"<p>Squashing your commits is to create a new commit with the net-result of several commits.  This effectively throws away the intermediate commit messages and the details about the specific changes to that commit.</p>"},{"location":"blog/2021/06/08/git-flow/#reset-commit","title":"Reset Commit","text":"<p>A corollary to a squash commit is to reset (<code>git reset --soft</code>) a current branch to a commit.  If you want to clean several commits before pushing, then you can reset to the commit just before the last one you want to remove.  Then, you can commit all of the changes that are left in your repository.</p>"},{"location":"blog/2021/06/08/git-flow/#rebase-commit","title":"Rebase Commit","text":"<p>It took me awhile to get my mind around rebase.  This probably stemmed from my lack of use of the word itself.  Instead of merging two branches, you can treat the other branch as the new base of your current branch.  Git will take each of your commits beyond the split of the two branches and reapply them to the rebased branch.  </p> <p>That's it!  Now, you can create clean and meaningful commit logs that you can use for debugging, blaming (shame-shame), or release tracking.  Get rid of those pesky commits!</p>"},{"location":"blog/2021/06/08/git-flow/#ps-beyond-git","title":"P.S. Beyond Git","text":"<ul> <li>Now enter pull request, merge request, and the like.  Systems such as Github and GitLab provide tooling to the heavy lifting for squashing and rebasing for you while also providing the ability to capture comments and other details to request.  </li> </ul>"},{"location":"blog/2021/12/03/git-flow-simplified/","title":"Git Flow Simplified","text":"<p>Oh, there are so many ways of managing git repositories.  I've written on this topic before, but as usual, things have changed.  I recently changed teams at work, and with a new team comes a new culture of devops.  </p>"},{"location":"blog/2021/12/03/git-flow-simplified/#tldr","title":"TL;DR","text":"<p>\"CI\" means to continuously integrate, so sync often.</p>"},{"location":"blog/2021/12/03/git-flow-simplified/#simple-git-flow","title":"Simple Git Flow","text":"<ol> <li>Keep to a single trunk (such as <code>/develop</code>)</li> <li>Commit on all logically complete units of code.</li> <li>Fetch from remote.</li> <li>Rebase.</li> <li>Push.</li> <li>Repeat &gt; 5x/day</li> </ol> <p>I usually script the above like this:</p> <p>sync.sh</p> <pre><code>#!/bin/sh\nmessage=$1\ngit add -A\ngit commit -m \"${message}\"\ngit fetch\ngit rebase\ngit push\n</code></pre>"},{"location":"blog/2021/12/03/git-flow-simplified/#ci-vs-branching","title":"CI vs Branching","text":"<p>By definition \"continuous integration\" and \"branching\" are antithetical to each other.  I used to think of \"CI\" as something the server does to integrate developers code.  But, I now see that the developers' systems are also a point of integration.  </p> <p>The goal is to integrate as quickly as possible (hence \"continuous\").  </p>"},{"location":"blog/2021/12/03/git-flow-simplified/#still-branch","title":"Still Branch?","text":"<p>Ok, ok, I have to be pragmatic and say that I still branch.  I now use branching for experimental coding, prototyping, and just figuring things out.  But I try to get off the branch within a couple of hours.  </p> <p>Quite frankily, if I knew everything about all code that I'll ever need to write, then I would avoid branching completely.  </p> <p>So, for branching, I usually script it such as this:</p> <p>branch.sh</p> <pre><code>#!/bin/sh\nbranch=$1\ngit checkout -b $branch\ngit push -u origin $branch\n</code></pre> <p>Then, when I'm ready to merge back to trunk (<code>/develop</code>) I complete my branch:</p> <p>complete.sh</p> <pre><code>#!/bin/sh\nmessage=$1\ngit add -A\ngit commit -m \"${message}\"\ngit push\ncurrent_branch=$(git branch --show-current)\ngit checkout develop\ngit merge --squash $current_branch\ngit commit -m \"${message}\"\ngit push $1\ngit branch -d $current_branch\ngit push origin --delete $current_branch\n</code></pre> <p>Note: I use <code>git merge --squash $current_branch</code> to keep the commits on trunk (<code>/develop</code>) clean.</p>"},{"location":"blog/2023/05/25/python-annotations/","title":"Dynamic Python","text":"<p>Python is great language to prototype ideas.  The dynamic runtime opens up some possibilities that are very difficult in statically typed languages.  </p> <p>However, there's a catch:  the dynamic runtime should not come at the cost of poor code documentation.  Let's take this simple, arbitrary function:</p> <pre><code>def transform(data):\n    '''Transform this data.'''\n\n    return [\n        item * idx\n        for idx, item in enumerate(data)\n    ]\n</code></pre> <p>The <code>transform</code> function above is pretty straightforward.  It multiplies the numbers in the list by their index, right?  Let's take it out for a spin:</p> <pre><code>print(transform([1, 2, 3]))\n# [0, 2, 6]\n</code></pre> <p>Yep!  That's simple.  But, wait, what about this?</p> <pre><code>print(transform([['a'], ['b'], ['c']]))\n[[], ['b'], ['c', 'c']]\n</code></pre> <p>What's going on here?  Well, nothing in the function said to explicitly work with integers.  How would you restrict the types of inputs so that you can reason about the behavior more simply?  </p> <p>Also, there's another problem:  the function signature doesn't inform you of it's behavior.  You have to read the function implementation to know what it does.  The function signature is the function name, it's arguments' names and types, and the return type.  We should be able to read the signature <code>def transform(data)</code> and know exactly what's going on.  This problem cannot be over-stated.</p> <p>Enter Python type annotations!  </p> <pre><code>def multiply_by_index(data: list[int]) -&gt; list[int]:\n    '''Multiply the numbers by their index.'''\n\n    return [\n        num * idx\n        for idx, num in enumerate(data)\n    ]\n</code></pre> <p>Now, the function signature is very informative:  <code>def multiply_by_index(data: list[int]) -&gt; list[int]</code>.  And, the types are restricted at code-time:</p> <p></p> <p>Note that this restriction only applies while coding; the runtime still behaves the same:</p> <pre><code>print(multiple_by_index([1, 2, 3]))\nprint(multiple_by_index([['a'], ['b'], ['c']]))\n# [0, 2, 6]\n# [[], ['b'], ['c', 'c']]\n</code></pre> <p>The runtime/code-time distinction might seem unhelpful, but I have experienced first-hand seeing the benefits of annotations in large codebases.  The clearer function signatures allow me to build up ever more complicated logic while ignoring the implementation details of the simpler functions.</p>"},{"location":"blog/2023/06/15/dockerize-your-development-today/","title":"Dockerize Your Development Today!","text":""},{"location":"blog/2023/06/15/dockerize-your-development-today/#no-more-it-works-on-my-machine-excuses","title":"No More \"It Works on My Machine\" Excuses","text":"<p>We've all been there. Code that works on my machine doesn't work on yours and vice versa. But, why?  There are like a number of issues to troubleshoot.  Some of the most common are:</p> <ol> <li>Different versions of the programming language, sdk, and runtime.</li> <li>Different package versions.</li> <li>Different operating systems.</li> <li>Different software toolchains.</li> <li>...</li> </ol> <p>The list goes on and on.</p>"},{"location":"blog/2023/06/15/dockerize-your-development-today/#docker-to-the-rescue","title":"Docker to the Rescue","text":"<p>Docker is great for containerizing your services in production.  But, we can do better.  Why not use it to containerize your development environment?  This way, you can be sure that your development environment matches your production environment exactly.  No more surprises, no more excuses.</p> <p>You can simply build a new image when anything on the list above changes and push it to shared container registry for the team.  Then, you can check in the image tag to version control and everyone can spin up an identical environment in minutes.  No more wasted time setting up dependencies and configuring tools.</p> <p>Check out vscode's dev containers for more information.</p> <p>Once I started using docker for development, I never looked back.  It's a game changer.</p>"},{"location":"blog/2023/12/10/use-a-run-script/","title":"Use a Run Script","text":""},{"location":"blog/2023/12/10/use-a-run-script/#why-run","title":"Why <code>run</code>?","text":"<p>This <code>run</code> script was inspired by <code>npm run</code> scripts.  Moving your development flow to a run script allows you to setup environments, install, or anything else you need to develop your project.  The <code>run</code> script also allows your to consolidate your scripts into a single script.  You'd likely have an <code>install.sh</code> script, another for <code>test.sh</code>, etc.</p>"},{"location":"blog/2023/12/10/use-a-run-script/#how-to-run","title":"How to <code>run</code>?","text":""},{"location":"blog/2023/12/10/use-a-run-script/#add-current-directory-to-path","title":"Add Current Directory to <code>$PATH</code>","text":"<p>To start, add the current directory to your <code>$PATH</code>.  I prefer to run scripts by name rather than with the relative path prefix (<code>./</code>).  So, I usually use <code>run</code> rather than <code>./run</code>.</p> <pre><code># Add to `~/.bashrc`\nexport PATH=\"$PATH:.\"\n</code></pre>"},{"location":"blog/2023/12/10/use-a-run-script/#add-the-script","title":"Add the Script","text":"<p>Setup a run script for quick development.</p> <p>Create the run script file:</p> <pre><code>$ touch run\n</code></pre> <p>Add the run script:</p> <pre><code>#!/bin/bash\n\nhelp() {\n    echo \"App\"\n    echo \"=========================\"\n    echo \"start     Run the application\"\n    echo \"test      Test the application\"\n}\n\n# Setup Tab Complete\n_tab_complete() {\n    local cur prev opts\n    opts=\"start test\"\n    COMPREPLY=()\n    cur=\"${COMP_WORDS[COMP_CWORD]}\"\n    prev=\"${COMP_WORDS[COMP_CWORD-1]}\"\n    COMPREPLY=( $(compgen -W \"${opts}\" -- ${cur}) )\n    return 0\n}\n\ncomplete -F _tab_complete run\n\ncmd=$1\n\n# Remove the first argument (shift left)\nshift\n\ncase $cmd in\n    \"start\")\n        echo \"TODO: Add [start]\"\n        # Use \"$@\" to pass all remaining arguments.\n        ;;\n    \"test\")\n        echo \"TODO: Add [test]\"\n        # Use \"$@\" to pass all remaining arguments.\n        ;;\n    *)\n        help\n        ;;\nesac\n</code></pre> <p>Example run:</p> <pre><code>$ . run # Source the run script to add tab complete.\nApp\n=========================\nstart     Run the application\ntest      Test the application\n$ run start\nTODO: Add [start]\n</code></pre>"},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/archive/2021/","title":"2021","text":""},{"location":"blog/archive/2020/","title":"2020","text":""},{"location":"blog/archive/2019/","title":"2019","text":""},{"location":"blog/category/devops/","title":"DevOps","text":""},{"location":"blog/category/bash/","title":"Bash","text":""},{"location":"blog/page/2/","title":"Blog","text":""},{"location":"blog/page/3/","title":"Blog","text":""},{"location":"blog/page/4/","title":"Blog","text":""},{"location":"blog/archive/2020/page/2/","title":"2020","text":""}]}